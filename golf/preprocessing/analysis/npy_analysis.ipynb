{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa35ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] skeleton_dataset_train_ids.npy\n",
      "- shape: (806,)\n",
      "- dtype: <U42\n",
      "- example (앞/뒤 3개): ['20201120_General_025_DOC_A_M40_MM_035_crop', '20201116_General_001_DOS_A_M40_MM_053_crop', '20201117_General_005_DOC_A_M40_MM_039_crop'] ... ['20201123_General_030_DOS_A_M40_MM_035_crop', '20201126_General_046_DOS_A_M30_MM_039_crop', '20201126_General_048_DOS_A_M30_MM_052_crop']\n",
      "- unique IDs: 806 / 806\n",
      "  ✅ 모든 ID가 정상\n",
      "\n",
      "[+] skeleton_dataset_valid_ids.npy\n",
      "- shape: (100,)\n",
      "- dtype: <U42\n",
      "- example (앞/뒤 3개): ['20201126_General_045_DOS_A_M30_MM_033_crop', '20201117_General_011_DOC_A_M40_BM_044_crop', '20201202_General_065_DOC_A_M40_MM_062_crop'] ... ['20201120_General_024_DOC_A_M40_MM_001_crop', '20201117_General_005_DOC_A_M40_MM_014_crop', '20201117_General_006_DOC_A_M40_BS_029_crop']\n",
      "- unique IDs: 100 / 100\n",
      "  ✅ 모든 ID가 정상\n",
      "\n",
      "[+] skeleton_dataset_test_ids.npy\n",
      "- shape: (102,)\n",
      "- dtype: <U42\n",
      "- example (앞/뒤 3개): ['20201130_General_058_DOS_A_M30_MM_017_crop', '20201120_General_027_DOC_A_M40_MM_062_crop', '20201130_General_058_DOS_A_M30_MM_009_crop'] ... ['20201127_General_049_DOS_A_M30_MM_061_crop', '20201201_General_063_DOS_A_M50_MS_056_crop', '20201126_General_048_DOS_A_M30_MM_059_crop']\n",
      "- unique IDs: 102 / 102\n",
      "  ✅ 모든 ID가 정상\n",
      "\n",
      "--- Cross check ---\n",
      "train ∩ valid : 0\n",
      "train ∩ test  : 0\n",
      "valid ∩ test  : 0\n",
      "전체 합집합(중복 제거): 1008\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 각 split별 파일 경로 지정 (수정해서 사용)\n",
    "splits = ['train', 'valid', 'test']\n",
    "base = Path(r\"D:\\golfDataset\\dataset\\crop_pkl\")  # 파일이 있는 디렉토리\n",
    "for split in splits:\n",
    "    npy_path = base / f'skeleton_dataset_{split}_ids.npy'\n",
    "    print(f\"\\n[+] {npy_path.name}\")\n",
    "    ids = np.load(npy_path, allow_pickle=True)\n",
    "    print(f\"- shape: {ids.shape}\")\n",
    "    print(f\"- dtype: {ids.dtype}\")\n",
    "    print(f\"- example (앞/뒤 3개): {ids[:3].tolist()} ... {ids[-3:].tolist()}\")\n",
    "    # 중복 체크\n",
    "    print(f\"- unique IDs: {len(set(ids))} / {len(ids)}\")\n",
    "    # None or 빈 값 체크\n",
    "    nulls = [i for i, v in enumerate(ids) if v is None or str(v).strip() == \"\"]\n",
    "    if nulls:\n",
    "        print(f\"  ⚠️ None/empty ID found at idx: {nulls[:10]} (총 {len(nulls)}개)\")\n",
    "    else:\n",
    "        print(\"  ✅ 모든 ID가 정상\")\n",
    "\n",
    "# 전체 교집합/차집합 분석 (train-valid-test)\n",
    "ids_train = np.load(base / 'skeleton_dataset_train_ids.npy', allow_pickle=True)\n",
    "ids_valid = np.load(base / 'skeleton_dataset_valid_ids.npy', allow_pickle=True)\n",
    "ids_test  = np.load(base / 'skeleton_dataset_test_ids.npy', allow_pickle=True)\n",
    "\n",
    "set_train = set(ids_train)\n",
    "set_valid = set(ids_valid)\n",
    "set_test  = set(ids_test)\n",
    "print(\"\\n--- Cross check ---\")\n",
    "print(f\"train ∩ valid : {len(set_train & set_valid)}\")\n",
    "print(f\"train ∩ test  : {len(set_train & set_test)}\")\n",
    "print(f\"valid ∩ test  : {len(set_valid & set_test)}\")\n",
    "\n",
    "all_ids = set_train | set_valid | set_test\n",
    "print(f\"전체 합집합(중복 제거): {len(all_ids)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier_fusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
