{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebcad6c",
   "metadata": {},
   "source": [
    "# CLS 토큰 only, fintuned 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca35136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2117개 mp4 처리\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:  48%|█████████████              | 1023/2117 [28:31<30:30,  1.67s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can't extend empty axis 0 using modes other than 'constant' or 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    139\u001b[39m out_path = PER_VIDEO_DIR / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    140\u001b[39m meta_path = PER_VIDEO_DIR / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m clips = \u001b[43mload_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clips\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCLIPS_PER_VID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m feats = []\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m clip \u001b[38;5;129;01min\u001b[39;00m clips:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mload_clip\u001b[39m\u001b[34m(path, num_clips)\u001b[39m\n\u001b[32m     54\u001b[39m clips = []\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s0, s1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(seg_edges[:-\u001b[32m1\u001b[39m], seg_edges[\u001b[32m1\u001b[39m:]):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     idx = \u001b[43muniform_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_FRAMES\u001b[49m\u001b[43m)\u001b[49m + s0\n\u001b[32m     57\u001b[39m     arr = vr.get_batch(idx).asnumpy().astype(np.uint8)  \u001b[38;5;66;03m# (T,H,W,3)\u001b[39;00m\n\u001b[32m     58\u001b[39m     clip = torch.from_numpy(arr).permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous() \u001b[38;5;66;03m# (T,C,H,W)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36muniform_sample\u001b[39m\u001b[34m(L, N)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m L >= N:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.linspace(\u001b[32m0\u001b[39m, L-\u001b[32m1\u001b[39m, N).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m-\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43medge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\numpy\\lib\\_arraypad_impl.py:827\u001b[39m, in \u001b[36mpad\u001b[39m\u001b[34m(array, pad_width, mode, **kwargs)\u001b[39m\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m axis, width_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, pad_width):\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m array.shape[axis] == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(width_pair):\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    828\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt extend empty axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m using modes other than \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    829\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mconstant\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mempty\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    830\u001b[39m             )\n\u001b[32m    831\u001b[39m     \u001b[38;5;66;03m# passed, don't need to do anything more as _pad_simple already\u001b[39;00m\n\u001b[32m    832\u001b[39m     \u001b[38;5;66;03m# returned the correct result\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33medge\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: can't extend empty axis 0 using modes other than 'constant' or 'empty'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from decord import VideoReader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# === 하이퍼파라미터 및 경로 통일 ===\n",
    "ROOT = Path(r'D:/golfDataset/dataset')\n",
    "FUSION_DIR = Path(r'D:/Jabez/golf/fusion')\n",
    "PER_VIDEO_DIR = FUSION_DIR / 'embedding_data' / 'timesformer' / 'per_video'\n",
    "PER_VIDEO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = Path(r'D:\\Jabez\\golf\\Timesformer_finetune\\best_timesformer.pth')\n",
    "PRETRAINED = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "NUM_FRAMES = 16\n",
    "CLIPS_PER_VID = 8\n",
    "IMG_SIZE = 224\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sys.path.append(r'D:/timesformer')\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# === finetune 코드와 동일한 전처리 정의 ===\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = transforms.functional.to_pil_image(f)\n",
    "        img = transforms.functional.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = transforms.functional.center_crop(img, IMG_SIZE)\n",
    "        t = transforms.functional.to_tensor(img)\n",
    "        t = transforms.functional.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "# === finetune 코드와 동일한 샘플링 함수 ===\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N:\n",
    "        return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "# === 수정된 load_clip 함수 ===\n",
    "def load_clip(path: Path, num_clips: int):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    # ⚠️ 수정된 부분: 비디오 프레임 수가 0일 때 early return\n",
    "    if L == 0:\n",
    "        print(f\"[경고] 프레임이 0인 비디오: {path}\")\n",
    "        return [] # 빈 리스트 반환\n",
    "    \n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)  # (T,H,W,3)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous() # (T,C,H,W)\n",
    "        processed_clip = eval_clip(clip) # (T,C,H,W)\n",
    "        clips.append(processed_clip.permute(1, 0, 2, 3)) # (C,T,H,W)\n",
    "    return clips\n",
    "\n",
    "# train, test 폴더 내 balanced_true/false/crop_video/*.mp4 모두 처리\n",
    "mapping = {'balanced_true': 1, 'false': 0}\n",
    "all_mp4s = []\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    split_root = ROOT / split\n",
    "    for cat, lbl in mapping.items():\n",
    "        vd = split_root / cat / 'crop_video'\n",
    "        if not vd.exists(): continue\n",
    "        for mp4 in vd.glob('*.mp4'):\n",
    "            all_mp4s.append((mp4, lbl, cat, split))\n",
    "\n",
    "print(f'총 {len(all_mp4s)}개 mp4 처리')\n",
    "\n",
    "# TimeSformerWithDropout 클래스 추가\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "class TimeSformerEmbed(nn.Module):\n",
    "    def __init__(self, model_path, img_size, num_frames, num_classes, pretrained_path):\n",
    "        super().__init__()\n",
    "        self.base = TimeSformer(\n",
    "            img_size=img_size,\n",
    "            num_frames=num_frames,\n",
    "            num_classes=num_classes,\n",
    "            attention_type='divided_space_time',\n",
    "            pretrained_model=str(pretrained_path)\n",
    "        )\n",
    "        \n",
    "        torch.serialization.add_safe_globals([\n",
    "            np._core.multiarray.scalar, \n",
    "            np.dtype, \n",
    "            np.dtypes.Float64DType\n",
    "        ])\n",
    "        \n",
    "        model_with_dropout = TimeSformerWithDropout(self.base, dropout_p=0.2)\n",
    "        \n",
    "        ckpt = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "        \n",
    "        model_with_dropout.load_state_dict(ckpt[\"model\"])\n",
    "        \n",
    "        self.base = model_with_dropout.base\n",
    "        self.base.head = nn.Identity()\n",
    "        self.base.cls_head = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "embed_model = TimeSformerEmbed(\n",
    "    model_path=MODEL_PATH,\n",
    "    img_size=IMG_SIZE,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    num_classes=2,\n",
    "    pretrained_path=PRETRAINED\n",
    ").to(DEVICE)\n",
    "embed_model.eval()\n",
    "\n",
    "# 임베딩 추출 및 저장\n",
    "for mp4, lbl, cat, split in tqdm(all_mp4s, desc='Extracting', ncols=80):\n",
    "    vid = mp4.stem\n",
    "    out_path = PER_VIDEO_DIR / f'{vid}.npy'\n",
    "    meta_path = PER_VIDEO_DIR / f'{vid}.json'\n",
    "    \n",
    "    clips = load_clip(mp4, num_clips=CLIPS_PER_VID)\n",
    "    \n",
    "    if not clips: # 비디오가 비어있으면 건너뜁니다\n",
    "        continue\n",
    "\n",
    "    feats = []\n",
    "    for clip in clips:\n",
    "        c = clip.unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = embed_model.base.model.forward_features(c)\n",
    "        cls = out[:,0,:] if out.ndim==3 else out\n",
    "        feats.append(cls.squeeze(0).cpu().numpy())\n",
    "    emb = np.stack(feats,0).mean(0)\n",
    "    np.save(out_path, emb)\n",
    "    meta = {\n",
    "        'video_id': vid, 'label': lbl, 'category': cat, 'split': split,\n",
    "        'mp4_path': str(mp4)\n",
    "    }\n",
    "    with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
