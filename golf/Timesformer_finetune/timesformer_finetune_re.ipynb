{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7802a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  현재 class_weight 사용 중입니다. oversample과 동시 사용은 피하세요.\n",
      "ℹ️  현재 class_weight 사용 중입니다. oversample과 동시 사용은 피하세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 653\u001b[39m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=(DEVICE==\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    652\u001b[39m     outs = model(vids)                                            \u001b[38;5;66;03m# [B*C, 2]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;66;03m# ✅ 누적: 스케일한 loss를 나눠서 backward\u001b[39;00m\n\u001b[32m    656\u001b[39m scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ======================= Config (한 곳에서 전부 관리) =======================\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings, random, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Paths\n",
    "TRAIN_ROOT      = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT       = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH    = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "EPOCHS_DIR      = Path(\"epochs\"); EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "BEST_CKPT       = Path(\"best_timesformer.pth\")\n",
    "\n",
    "# ---- Data / Model\n",
    "IMG_SIZE               = 224\n",
    "NUM_FRAMES             = 16\n",
    "TRAIN_CLIPS_PER_VIDEO  = 1\n",
    "EVAL_CLIPS_PER_VIDEO   = 8   # 필요시 5~10으로 올리면 변동성↓(시간 비용↑)\n",
    "\n",
    "# ---- Train hyperparams\n",
    "BATCH_SIZE     = 2\n",
    "WORKERS        = 0          # Windows면 2~4 권장, Linux면 더 높여도 OK\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 0.01\n",
    "DROPOUT        = 0.2\n",
    "TRAIN_BLOCK    = 2           # -1이면 전층 학습\n",
    "VAL_SPLIT      = 0.10        # train -> train/val\n",
    "BALANCE_METHOD = \"class_weight\"   # 'none' | 'oversample' | 'class_weight'\n",
    "EPOCHS         = 20\n",
    "WARMUP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "EARLY_STOP     = 5\n",
    "LABEL_SMOOTH   = 0.10\n",
    "SEED           = 42\n",
    "\n",
    "# (선택) grad accumulation 추가 시\n",
    "GRAD_ACCUM_STEPS = 2     # 유효 배치 = BATCH_SIZE * GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# ---- Logs / UI\n",
    "CLEAN_OUTPUT         = True\n",
    "SHOW_TRAIN_BAR       = True\n",
    "SHOW_EVAL_BAR        = True\n",
    "PLOT_CM_EVERY_EPOCH  = True\n",
    "PRINT_EPOCH_SAVE     = True\n",
    "\n",
    "# ======================= Imports (한 번만) =======================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ======================= Repro / Device =======================\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======================= Augment / Sampling =======================\n",
    "def _sample_clip_params(first_frame_tensor):\n",
    "    scale = (0.8, 1.0)\n",
    "    i, j, h, w = T.RandomResizedCrop.get_params(first_frame_tensor, scale=scale, ratio=(3/4, 4/3))\n",
    "    do_hflip = random.random() < 0.5\n",
    "    cj = T.ColorJitter(0.2, 0.2, 0.2, 0.0)\n",
    "    fn_idx, b, c, s, h2 = T.ColorJitter.get_params(cj.brightness, cj.contrast, cj.saturation, cj.hue)\n",
    "    return (i, j, h, w), do_hflip, (fn_idx, b, c, s)\n",
    "\n",
    "def augment_clip(frames):  # frames: [T, C, H, W]\n",
    "    (i, j, h, w), do_hflip, (fn_idx, b, c, s) = _sample_clip_params(frames[0])\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=InterpolationMode.BICUBIC)\n",
    "        if do_hflip: img = TF.hflip(img)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0: img = TF.adjust_brightness(img, b)\n",
    "            elif fn_id == 1: img = TF.adjust_contrast(img, c)\n",
    "            elif fn_id == 2: img = TF.adjust_saturation(img, s)\n",
    "            elif fn_id == 3: pass\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = TF.center_crop(img, IMG_SIZE)\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N: return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path, num_clips: int, train=True):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    if L == 0:\n",
    "        print(f\"[경고] 프레임이 0인 비디오: {path}\")\n",
    "        # 빈 텐서 반환 (혹은 raise Exception)\n",
    "        return [torch.zeros((3, NUM_FRAMES, IMG_SIZE, IMG_SIZE), dtype=torch.float32) for _ in range(num_clips)]\n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous()\n",
    "        proc = augment_clip(clip) if train else eval_clip(clip)\n",
    "        clips.append(proc.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "\n",
    "# ======================= Dataset / Loader =======================\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance=\"none\", train=True, verbose=True):\n",
    "        self.train = train\n",
    "        true_samples, false_samples = [], []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            mp4_dir = (root / sub / \"crop_video\")\n",
    "            if mp4_dir.exists():\n",
    "                for p in mp4_dir.glob(\"*.mp4\"):\n",
    "                    (true_samples if label==1 else false_samples).append((p, label))\n",
    "\n",
    "        if balance == \"oversample\":\n",
    "            n_true, n_false = len(true_samples), len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor, remainder = n_true // n_false, n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        if verbose:\n",
    "            print(f\"✅ {len(self.samples)} samples found in {root} (balance='{balance}')\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        num_clips = TRAIN_CLIPS_PER_VIDEO if self.train else EVAL_CLIPS_PER_VIDEO\n",
    "        clips = load_clip(path, num_clips=num_clips, train=self.train)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "def filesystem_class_weights(train_root: Path, num_classes=2):\n",
    "    true_cnt  = len(list((train_root/\"balanced_true\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    false_cnt = len(list((train_root/\"false\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    cnts = {1: true_cnt, 0: false_cnt}\n",
    "    total = sum(cnts.values())\n",
    "    if total == 0: return torch.ones(num_classes, dtype=torch.float32)\n",
    "    w = [total / max(1, cnts.get(c, 0)) for c in range(num_classes)]\n",
    "    s = sum(w); w = [wi * (num_classes / s) for wi in w]\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# oversample + class_weight 동시 사용 경고 (과벌점 위험)\n",
    "if BALANCE_METHOD == \"oversample\":\n",
    "    print(\"⚠️  현재 oversample 사용 중입니다. val/test는 항상 balance='none'으로 평가합니다.\")\n",
    "elif BALANCE_METHOD == \"class_weight\":\n",
    "    print(\"ℹ️  현재 class_weight 사용 중입니다. oversample과 동시 사용은 피하세요.\")\n",
    "\n",
    "# 1) 전체 트레인 세트(증강/오버샘플 옵션 포함)\n",
    "train_full = GolfSwingDataset(\n",
    "    TRAIN_ROOT,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\"),\n",
    "    train=True,\n",
    "    verbose=not CLEAN_OUTPUT\n",
    ")\n",
    "\n",
    "# 2) 인덱스 분할만 얻기\n",
    "val_len = max(1, int(len(train_full) * VAL_SPLIT))\n",
    "train_len = len(train_full) - val_len\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(train_full, [train_len, val_len], generator=g)\n",
    "\n",
    "# 3) 동일한 샘플 목록을 공유하되, 서로 다른 'train 플래그'를 갖는 별도 Dataset 인스턴스 생성\n",
    "def clone_dataset_with_indices(src_ds: GolfSwingDataset, indices, train_flag: bool, balance: str):\n",
    "    ds = GolfSwingDataset(TRAIN_ROOT, balance=balance, train=train_flag, verbose=False)\n",
    "    ds.samples = [src_ds.samples[i] for i in indices]   # 원본에서 선택된 샘플만 복사\n",
    "    return ds\n",
    "\n",
    "train_dataset = clone_dataset_with_indices(\n",
    "    train_full, train_subset.indices, train_flag=True,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\")\n",
    ")\n",
    "val_dataset   = clone_dataset_with_indices(\n",
    "    train_full, val_subset.indices,   train_flag=False, balance=\"none\"\n",
    ")\n",
    "\n",
    "# 4) 테스트 세트는 원래대로 (평가 전용이므로 train=False)\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=\"none\", train=False, verbose=not CLEAN_OUTPUT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# ======================= Model / Optim =======================\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "base_model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES, num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PTH)\n",
    ").to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=DROPOUT).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model, train_blocks: int):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    if train_blocks == -1:\n",
    "        for p in base.parameters(): p.requires_grad = True\n",
    "        return [p for p in base.parameters() if p.requires_grad]\n",
    "    # Freeze all\n",
    "    for p in base.parameters(): p.requires_grad = False\n",
    "    # Unfreeze head\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    if head is not None:\n",
    "        for p in head.parameters(): p.requires_grad = True\n",
    "    # Unfreeze last N blocks\n",
    "    blocks = getattr(inner, \"blocks\", None)\n",
    "    if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "        total = len(blocks); start = max(0, total - train_blocks)\n",
    "        for b in blocks[start:]:\n",
    "            for p in b.parameters(): p.requires_grad = True\n",
    "    return [p for p in base.parameters() if p.requires_grad]\n",
    "\n",
    "def build_optimizer(model, train_blocks: int, base_lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    head_params = list(head.parameters()) if head is not None else []\n",
    "\n",
    "    if train_blocks == -1:\n",
    "        head_set = set(head_params)\n",
    "        backbone_params = [p for p in base.parameters() if p.requires_grad and p not in head_set]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,     \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * 0.5, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    else:\n",
    "        blocks = getattr(inner, \"blocks\", None)\n",
    "        block_params = []\n",
    "        if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "            total = len(blocks); start = max(0, total - train_blocks)\n",
    "            for b in blocks[start:]:\n",
    "                block_params += [p for p in b.parameters() if p.requires_grad]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,  \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": block_params, \"lr\": base_lr * 1.0, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(param_groups)\n",
    "    # === LR 스케줄러가 그룹 비율을 유지하도록 initial_lr 저장 ===\n",
    "    for g in opt.param_groups:\n",
    "        g[\"initial_lr\"] = g[\"lr\"]\n",
    "    return opt\n",
    "\n",
    "trainable_params = get_trainable_params(model, TRAIN_BLOCK)\n",
    "optimizer = build_optimizer(model, TRAIN_BLOCK, base_lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class weights (옵션)\n",
    "if BALANCE_METHOD == \"class_weight\":\n",
    "    class_w = filesystem_class_weights(TRAIN_ROOT).to(DEVICE)\n",
    "else:\n",
    "    class_w = None\n",
    "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# AMP & LR schedule — 그룹 비율 유지형 스케일 팩터\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def get_lr_factor(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    t = epoch - WARMUP_EPOCHS\n",
    "    T_total = max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1 + np.cos(np.pi * t / T_total))\n",
    "\n",
    "def set_lr_with_factor(optimr, factor):\n",
    "    for g in optimr.param_groups:\n",
    "        g['lr'] = g['initial_lr'] * factor\n",
    "\n",
    "# ======================= Train / Eval =======================\n",
    "def video_level_from_logits(logits, B, clips_per_video):\n",
    "    return logits.view(B, clips_per_video, -1).mean(dim=1)\n",
    "\n",
    "# ======================= Config (한 곳에서 전부 관리) =======================\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings, random, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Paths\n",
    "TRAIN_ROOT      = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT       = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH    = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "EPOCHS_DIR      = Path(\"epochs\"); EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "BEST_CKPT       = Path(\"best_timesformer.pth\")\n",
    "\n",
    "# ---- Data / Model\n",
    "IMG_SIZE               = 224\n",
    "NUM_FRAMES             = 16\n",
    "TRAIN_CLIPS_PER_VIDEO  = 1\n",
    "EVAL_CLIPS_PER_VIDEO   = 8   # 필요시 5~10으로 올리면 변동성↓(시간 비용↑)\n",
    "\n",
    "# ---- Train hyperparams\n",
    "BATCH_SIZE     = 2\n",
    "WORKERS        = 0          # Windows면 2~4 권장, Linux면 더 높여도 OK\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 0.01\n",
    "DROPOUT        = 0.2\n",
    "TRAIN_BLOCK    = 2           # -1이면 전층 학습\n",
    "VAL_SPLIT      = 0.10        # train -> train/val\n",
    "BALANCE_METHOD = \"class_weight\"   # 'none' | 'oversample' | 'class_weight'\n",
    "EPOCHS         = 20\n",
    "WARMUP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "EARLY_STOP     = 5\n",
    "LABEL_SMOOTH   = 0.10\n",
    "SEED           = 42\n",
    "\n",
    "# (선택) grad accumulation 추가 시\n",
    "GRAD_ACCUM_STEPS = 2     # 유효 배치 = BATCH_SIZE * GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# ---- Logs / UI\n",
    "CLEAN_OUTPUT         = True\n",
    "SHOW_TRAIN_BAR       = True\n",
    "SHOW_EVAL_BAR        = True\n",
    "PLOT_CM_EVERY_EPOCH  = True\n",
    "PRINT_EPOCH_SAVE     = True\n",
    "\n",
    "# ======================= Imports (한 번만) =======================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ======================= Repro / Device =======================\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======================= Augment / Sampling =======================\n",
    "def _sample_clip_params(first_frame_tensor):\n",
    "    scale = (0.8, 1.0)\n",
    "    i, j, h, w = T.RandomResizedCrop.get_params(first_frame_tensor, scale=scale, ratio=(3/4, 4/3))\n",
    "    do_hflip = random.random() < 0.5\n",
    "    cj = T.ColorJitter(0.2, 0.2, 0.2, 0.0)\n",
    "    fn_idx, b, c, s, h2 = T.ColorJitter.get_params(cj.brightness, cj.contrast, cj.saturation, cj.hue)\n",
    "    return (i, j, h, w), do_hflip, (fn_idx, b, c, s)\n",
    "\n",
    "def augment_clip(frames):  # frames: [T, C, H, W]\n",
    "    (i, j, h, w), do_hflip, (fn_idx, b, c, s) = _sample_clip_params(frames[0])\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=InterpolationMode.BICUBIC)\n",
    "        if do_hflip: img = TF.hflip(img)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0: img = TF.adjust_brightness(img, b)\n",
    "            elif fn_id == 1: img = TF.adjust_contrast(img, c)\n",
    "            elif fn_id == 2: img = TF.adjust_saturation(img, s)\n",
    "            elif fn_id == 3: pass\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = TF.center_crop(img, IMG_SIZE)\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N: return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path, num_clips: int, train=True):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    if L == 0:\n",
    "        print(f\"[경고] 프레임이 0인 비디오: {path}\")\n",
    "        # 빈 텐서 반환 (혹은 raise Exception)\n",
    "        return [torch.zeros((3, NUM_FRAMES, IMG_SIZE, IMG_SIZE), dtype=torch.float32) for _ in range(num_clips)]\n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous()\n",
    "        proc = augment_clip(clip) if train else eval_clip(clip)\n",
    "        clips.append(proc.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "\n",
    "# ======================= Dataset / Loader =======================\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance=\"none\", train=True, verbose=True):\n",
    "        self.train = train\n",
    "        true_samples, false_samples = [], []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            mp4_dir = (root / sub / \"crop_video\")\n",
    "            if mp4_dir.exists():\n",
    "                for p in mp4_dir.glob(\"*.mp4\"):\n",
    "                    (true_samples if label==1 else false_samples).append((p, label))\n",
    "\n",
    "        if balance == \"oversample\":\n",
    "            n_true, n_false = len(true_samples), len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor, remainder = n_true // n_false, n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        if verbose:\n",
    "            print(f\"✅ {len(self.samples)} samples found in {root} (balance='{balance}')\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        num_clips = TRAIN_CLIPS_PER_VIDEO if self.train else EVAL_CLIPS_PER_VIDEO\n",
    "        clips = load_clip(path, num_clips=num_clips, train=self.train)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "def filesystem_class_weights(train_root: Path, num_classes=2):\n",
    "    true_cnt  = len(list((train_root/\"balanced_true\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    false_cnt = len(list((train_root/\"false\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    cnts = {1: true_cnt, 0: false_cnt}\n",
    "    total = sum(cnts.values())\n",
    "    if total == 0: return torch.ones(num_classes, dtype=torch.float32)\n",
    "    w = [total / max(1, cnts.get(c, 0)) for c in range(num_classes)]\n",
    "    s = sum(w); w = [wi * (num_classes / s) for wi in w]\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# oversample + class_weight 동시 사용 경고 (과벌점 위험)\n",
    "if BALANCE_METHOD == \"oversample\":\n",
    "    print(\"⚠️  현재 oversample 사용 중입니다. val/test는 항상 balance='none'으로 평가합니다.\")\n",
    "elif BALANCE_METHOD == \"class_weight\":\n",
    "    print(\"ℹ️  현재 class_weight 사용 중입니다. oversample과 동시 사용은 피하세요.\")\n",
    "\n",
    "# 1) 전체 트레인 세트(증강/오버샘플 옵션 포함)\n",
    "train_full = GolfSwingDataset(\n",
    "    TRAIN_ROOT,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\"),\n",
    "    train=True,\n",
    "    verbose=not CLEAN_OUTPUT\n",
    ")\n",
    "\n",
    "# 2) 인덱스 분할만 얻기\n",
    "val_len = max(1, int(len(train_full) * VAL_SPLIT))\n",
    "train_len = len(train_full) - val_len\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(train_full, [train_len, val_len], generator=g)\n",
    "\n",
    "# 3) 동일한 샘플 목록을 공유하되, 서로 다른 'train 플래그'를 갖는 별도 Dataset 인스턴스 생성\n",
    "def clone_dataset_with_indices(src_ds: GolfSwingDataset, indices, train_flag: bool, balance: str):\n",
    "    ds = GolfSwingDataset(TRAIN_ROOT, balance=balance, train=train_flag, verbose=False)\n",
    "    ds.samples = [src_ds.samples[i] for i in indices]   # 원본에서 선택된 샘플만 복사\n",
    "    return ds\n",
    "\n",
    "train_dataset = clone_dataset_with_indices(\n",
    "    train_full, train_subset.indices, train_flag=True,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\")\n",
    ")\n",
    "val_dataset   = clone_dataset_with_indices(\n",
    "    train_full, val_subset.indices,   train_flag=False, balance=\"none\"\n",
    ")\n",
    "\n",
    "# 4) 테스트 세트는 원래대로 (평가 전용이므로 train=False)\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=\"none\", train=False, verbose=not CLEAN_OUTPUT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# ======================= Model / Optim =======================\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "base_model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES, num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PTH)\n",
    ").to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=DROPOUT).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model, train_blocks: int):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    if train_blocks == -1:\n",
    "        for p in base.parameters(): p.requires_grad = True\n",
    "        return [p for p in base.parameters() if p.requires_grad]\n",
    "    # Freeze all\n",
    "    for p in base.parameters(): p.requires_grad = False\n",
    "    # Unfreeze head\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    if head is not None:\n",
    "        for p in head.parameters(): p.requires_grad = True\n",
    "    # Unfreeze last N blocks\n",
    "    blocks = getattr(inner, \"blocks\", None)\n",
    "    if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "        total = len(blocks); start = max(0, total - train_blocks)\n",
    "        for b in blocks[start:]:\n",
    "            for p in b.parameters(): p.requires_grad = True\n",
    "    return [p for p in base.parameters() if p.requires_grad]\n",
    "\n",
    "def build_optimizer(model, train_blocks: int, base_lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    head_params = list(head.parameters()) if head is not None else []\n",
    "\n",
    "    if train_blocks == -1:\n",
    "        head_set = set(head_params)\n",
    "        backbone_params = [p for p in base.parameters() if p.requires_grad and p not in head_set]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,     \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * 0.5, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    else:\n",
    "        blocks = getattr(inner, \"blocks\", None)\n",
    "        block_params = []\n",
    "        if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "            total = len(blocks); start = max(0, total - train_blocks)\n",
    "            for b in blocks[start:]:\n",
    "                block_params += [p for p in b.parameters() if p.requires_grad]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,  \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": block_params, \"lr\": base_lr * 1.0, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(param_groups)\n",
    "    # === LR 스케줄러가 그룹 비율을 유지하도록 initial_lr 저장 ===\n",
    "    for g in opt.param_groups:\n",
    "        g[\"initial_lr\"] = g[\"lr\"]\n",
    "    return opt\n",
    "\n",
    "trainable_params = get_trainable_params(model, TRAIN_BLOCK)\n",
    "optimizer = build_optimizer(model, TRAIN_BLOCK, base_lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class weights (옵션)\n",
    "if BALANCE_METHOD == \"class_weight\":\n",
    "    class_w = filesystem_class_weights(TRAIN_ROOT).to(DEVICE)\n",
    "else:\n",
    "    class_w = None\n",
    "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# AMP & LR schedule — 그룹 비율 유지형 스케일 팩터\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def get_lr_factor(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    t = epoch - WARMUP_EPOCHS\n",
    "    T_total = max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1 + np.cos(np.pi * t / T_total))\n",
    "\n",
    "def set_lr_with_factor(optimr, factor):\n",
    "    for g in optimr.param_groups:\n",
    "        g['lr'] = g['initial_lr'] * factor\n",
    "\n",
    "# ======================= Train / Eval =======================\n",
    "def video_level_from_logits(logits, B, clips_per_video):\n",
    "    return logits.view(B, clips_per_video, -1).mean(dim=1)\n",
    "\n",
    "train_logs = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": []}\n",
    "val_acc_per_epoch, best_val_acc, patience = [], -1.0, 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    lr_factor = get_lr_factor(epoch)\n",
    "    set_lr_with_factor(optimizer, lr_factor)\n",
    "\n",
    "    total_videos = correct_videos = 0\n",
    "    running_loss, running_steps = 0.0, 0\n",
    "\n",
    "    bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\",\n",
    "            ncols=80, leave=False, disable=not SHOW_TRAIN_BAR)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)  # ✅ 루프 시작 전에 1번만\n",
    "\n",
    "    for step, (clips, label) in enumerate(bar):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)               # [B*C, C,T,H,W]\n",
    "        labs = label.repeat_interleave(TRAIN_CLIPS_PER_VIDEO).to(DEVICE) # 라벨 확장\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)                                            # [B*C, 2]\n",
    "            loss = criterion(outs, labs)\n",
    "\n",
    "        # ✅ 누적: 스케일한 loss를 나눠서 backward\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "\n",
    "        # 로깅은 원래 loss 기준으로\n",
    "        running_loss += loss.item()\n",
    "        running_steps += 1\n",
    "\n",
    "        # 비디오 단위 정확도 갱신(로짓 평균)\n",
    "        with torch.no_grad():\n",
    "            vid_logits = video_level_from_logits(outs, B, TRAIN_CLIPS_PER_VIDEO)\n",
    "            preds = vid_logits.argmax(1).cpu()\n",
    "            correct_videos += (preds == label).sum().item()\n",
    "            total_videos   += B\n",
    "\n",
    "        # ✅ 누적 step마다만 step/zero_grad\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # AMP 사용 시, clip 전에 unscale 권장\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if SHOW_TRAIN_BAR:\n",
    "            bar.set_postfix({\n",
    "                \"Loss\": f\"{running_loss/max(1,running_steps):.4f}\",\n",
    "                \"Acc\":  f\"{correct_videos/max(1,total_videos):.2%}\"\n",
    "            })\n",
    "\n",
    "    # ✅ 마지막에 남은 그라디언트 처리(미완 배치가 있을 때)\n",
    "    remainder = (step + 1) % GRAD_ACCUM_STEPS\n",
    "    if remainder != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    train_acc = correct_videos / max(1, total_videos)\n",
    "    avg_loss  = running_loss / max(1, running_steps)\n",
    "    train_logs[\"train_acc\"].append(train_acc); train_logs[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval(); all_preds, all_labels = [], []\n",
    "    val_total = val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        vbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\",\n",
    "                    ncols=80, leave=False, disable=not SHOW_EVAL_BAR)\n",
    "        for clips, label in vbar:\n",
    "            B = clips.shape[0]\n",
    "            vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "            with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "                outs = model(vids)\n",
    "                vid_logits = video_level_from_logits(outs, B, EVAL_CLIPS_PER_VIDEO)\n",
    "                pred = vid_logits.argmax(1).cpu()\n",
    "            val_total += B; val_correct += (pred == label).sum().item()\n",
    "            if SHOW_EVAL_BAR: vbar.set_postfix({\"Acc\": f\"{val_correct/max(1,val_total):.2%}\"})\n",
    "            all_preds.extend(pred.numpy()); all_labels.extend(label.numpy())\n",
    "\n",
    "    val_acc = val_correct / max(1, val_total)\n",
    "    train_logs[\"val_acc\"].append(val_acc); val_acc_per_epoch.append(val_acc)\n",
    "\n",
    "    if PLOT_CM_EVERY_EPOCH:\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "        disp.plot(cmap=\"Blues\"); plt.title(f\"Val Confusion Matrix (Epoch {epoch+1})\"); plt.show()\n",
    "\n",
    "    # ---- Early stop / Save best (val 기준)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc, patience = val_acc, 0\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict(),\n",
    "                    \"opt\": optimizer.state_dict(), \"metrics\": {\"val_acc\": val_acc}}, BEST_CKPT)\n",
    "        if not CLEAN_OUTPUT or PRINT_EPOCH_SAVE:\n",
    "            print(f\"  ✅ Best updated: val_acc={val_acc:.3%} -> saved to {BEST_CKPT}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= EARLY_STOP:\n",
    "            print(\"⛳ Early stopping triggered.\"); break\n",
    "\n",
    "    if CLEAN_OUTPUT:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | train_acc={train_acc:.3%} loss={avg_loss:.4f} \"\n",
    "              f\"| val_acc={val_acc:.3%} | best={best_val_acc:.3%}\")\n",
    "\n",
    "    with open(\"train_logs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_logs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]:  56%|██████████████████▍              | 144/257 [04:59<03:45,  1.99s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================= Final Test (정확도 + 불균형 지표) =======================\n",
    "# 필요시 아래 주석 해제 (학습 후 바로 테스트할 경우 생략 가능)\n",
    "model.load_state_dict(torch.load(BEST_CKPT)[\"model\"])\n",
    "\n",
    "model.eval(); all_preds, all_labels, all_scores = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(test_loader, desc=\"[Test]\", ncols=80, disable=not SHOW_EVAL_BAR):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)  # [B*C, 2]\n",
    "            # 동적으로 clips_per_video 계산\n",
    "            clips_per_video = outs.shape[0] // B\n",
    "            vid_logits = outs.view(B, clips_per_video, -1).mean(dim=1)  # [B,2]\n",
    "            probs = F.softmax(vid_logits, dim=1)  # 확률 스코어\n",
    "            pred = vid_logits.argmax(1).cpu()\n",
    "\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "        all_scores.extend(probs[:, 1].cpu().numpy())  # 양성( true ) 확률, 필요 시 false면 [:,0]\n",
    "\n",
    "# 원래 accuracy\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n✅ Test Accuracy: {test_acc:.3%}\")\n",
    "\n",
    "# 불균형 친화 지표\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "# ROC-AUC: 양성 클래스를 1로 가정\n",
    "try:\n",
    "    roc_auc = roc_auc_score(all_labels, np.array(all_scores))\n",
    "except ValueError:\n",
    "    roc_auc = float('nan')\n",
    "# PR-AUC (양성 기준)\n",
    "ap = average_precision_score(all_labels, np.array(all_scores))\n",
    "\n",
    "print(f\"🔎 Balanced Acc : {bal_acc:.3%}\")\n",
    "print(f\"🔎 Macro F1     : {macro_f1:.3%}\")\n",
    "print(f\"🔎 ROC-AUC      : {roc_auc:.3f}\")\n",
    "print(f\"🔎 PR-AUC(AP)   : {ap:.3f}\")\n",
    "\n",
    "# 클래스별 정밀도/재현율/F1\n",
    "prec, rec, f1, supp = precision_recall_fscore_support(all_labels, all_preds, labels=[0,1], zero_division=0)\n",
    "print(\"\\n[Per-class]\")\n",
    "print(f\" false(0) -> P:{prec[0]:.3f} R:{rec[0]:.3f} F1:{f1[0]:.3f} (n={supp[0]})\")\n",
    "print(f\" true (1) -> P:{prec[1]:.3f} R:{rec[1]:.3f} F1:{f1[1]:.3f} (n={supp[1]})\")\n",
    "\n",
    "# 정규화된 혼동행렬(비율)\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1], normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\"); plt.title(\"Test CM (Normalized)\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432714d",
   "metadata": {},
   "source": [
    "# true, false 1대1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# ======================= 1:1 (false:true) Test Evaluation =======================\n",
    "model.load_state_dict(torch.load(BEST_CKPT)[\"model\"])\n",
    "\n",
    "# 1. test_dataset에서 false/true 인덱스 분리\n",
    "false_indices = [i for i, (p, l) in enumerate(test_dataset.samples) if l == 0]\n",
    "true_indices  = [i for i, (p, l) in enumerate(test_dataset.samples) if l == 1]\n",
    "\n",
    "# 2. false 개수만큼 true에서 랜덤 추출\n",
    "random.seed(SEED)\n",
    "n_false = len(false_indices)\n",
    "sampled_true_indices = random.sample(true_indices, min(n_false, len(true_indices)))\n",
    "\n",
    "# 3. 합쳐서 1:1 인덱스 리스트 생성 및 셔플\n",
    "balanced_indices = false_indices + sampled_true_indices\n",
    "random.shuffle(balanced_indices)\n",
    "\n",
    "# 4. Subset DataLoader 생성\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "balanced_subset = Subset(test_dataset, balanced_indices)\n",
    "balanced_loader = DataLoader(balanced_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# 5. 평가\n",
    "model.eval(); all_preds, all_labels, all_scores = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(balanced_loader, desc=\"[Test 1:1]\", ncols=80, disable=not SHOW_EVAL_BAR):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)\n",
    "            clips_per_video = outs.shape[0] // B\n",
    "            vid_logits = outs.view(B, clips_per_video, -1).mean(dim=1)\n",
    "            probs = F.softmax(vid_logits, dim=1)\n",
    "            pred = vid_logits.argmax(1).cpu()\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "        all_scores.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "# 6. 지표 출력\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "try:\n",
    "    roc_auc = roc_auc_score(all_labels, np.array(all_scores))\n",
    "except ValueError:\n",
    "    roc_auc = float('nan')\n",
    "ap = average_precision_score(all_labels, np.array(all_scores))\n",
    "\n",
    "print(f\"\\n✅ [1:1] Test Accuracy: {test_acc:.3%}\")\n",
    "print(f\"🔎 Balanced Acc : {bal_acc:.3%}\")\n",
    "print(f\"🔎 Macro F1     : {macro_f1:.3%}\")\n",
    "print(f\"🔎 ROC-AUC      : {roc_auc:.3f}\")\n",
    "print(f\"🔎 PR-AUC(AP)   : {ap:.3f}\")\n",
    "\n",
    "prec, rec, f1, supp = precision_recall_fscore_support(all_labels, all_preds, labels=[0,1], zero_division=0)\n",
    "print(\"\\n[Per-class]\")\n",
    "print(f\" false(0) -> P:{prec[0]:.3f} R:{rec[0]:.3f} F1:{f1[0]:.3f} (n={supp[0]})\")\n",
    "print(f\" true (1) -> P:{prec[1]:.3f} R:{rec[1]:.3f} F1:{f1[1]:.3f} (n={supp[1]})\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1], normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\"); plt.title(\"Test 1:1 CM (Normalized)\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
