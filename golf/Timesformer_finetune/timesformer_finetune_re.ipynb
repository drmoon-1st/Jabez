{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7802a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\n",
      "â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 653\u001b[39m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=(DEVICE==\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    652\u001b[39m     outs = model(vids)                                            \u001b[38;5;66;03m# [B*C, 2]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;66;03m# âœ… ëˆ„ì : ìŠ¤ì¼€ì¼í•œ lossë¥¼ ë‚˜ëˆ ì„œ backward\u001b[39;00m\n\u001b[32m    656\u001b[39m scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ======================= Config (í•œ ê³³ì—ì„œ ì „ë¶€ ê´€ë¦¬) =======================\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings, random, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Paths\n",
    "TRAIN_ROOT      = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT       = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH    = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "EPOCHS_DIR      = Path(\"epochs\"); EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "BEST_CKPT       = Path(\"best_timesformer.pth\")\n",
    "\n",
    "# ---- Data / Model\n",
    "IMG_SIZE               = 224\n",
    "NUM_FRAMES             = 16\n",
    "TRAIN_CLIPS_PER_VIDEO  = 1\n",
    "EVAL_CLIPS_PER_VIDEO   = 8   # í•„ìš”ì‹œ 5~10ìœ¼ë¡œ ì˜¬ë¦¬ë©´ ë³€ë™ì„±â†“(ì‹œê°„ ë¹„ìš©â†‘)\n",
    "\n",
    "# ---- Train hyperparams\n",
    "BATCH_SIZE     = 2\n",
    "WORKERS        = 0          # Windowsë©´ 2~4 ê¶Œì¥, Linuxë©´ ë” ë†’ì—¬ë„ OK\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 0.01\n",
    "DROPOUT        = 0.2\n",
    "TRAIN_BLOCK    = 2           # -1ì´ë©´ ì „ì¸µ í•™ìŠµ\n",
    "VAL_SPLIT      = 0.10        # train -> train/val\n",
    "BALANCE_METHOD = \"class_weight\"   # 'none' | 'oversample' | 'class_weight'\n",
    "EPOCHS         = 20\n",
    "WARMUP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "EARLY_STOP     = 5\n",
    "LABEL_SMOOTH   = 0.10\n",
    "SEED           = 42\n",
    "\n",
    "# (ì„ íƒ) grad accumulation ì¶”ê°€ ì‹œ\n",
    "GRAD_ACCUM_STEPS = 2     # ìœ íš¨ ë°°ì¹˜ = BATCH_SIZE * GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# ---- Logs / UI\n",
    "CLEAN_OUTPUT         = True\n",
    "SHOW_TRAIN_BAR       = True\n",
    "SHOW_EVAL_BAR        = True\n",
    "PLOT_CM_EVERY_EPOCH  = True\n",
    "PRINT_EPOCH_SAVE     = True\n",
    "\n",
    "# ======================= Imports (í•œ ë²ˆë§Œ) =======================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ======================= Repro / Device =======================\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======================= Augment / Sampling =======================\n",
    "def _sample_clip_params(first_frame_tensor):\n",
    "    scale = (0.8, 1.0)\n",
    "    i, j, h, w = T.RandomResizedCrop.get_params(first_frame_tensor, scale=scale, ratio=(3/4, 4/3))\n",
    "    do_hflip = random.random() < 0.5\n",
    "    cj = T.ColorJitter(0.2, 0.2, 0.2, 0.0)\n",
    "    fn_idx, b, c, s, h2 = T.ColorJitter.get_params(cj.brightness, cj.contrast, cj.saturation, cj.hue)\n",
    "    return (i, j, h, w), do_hflip, (fn_idx, b, c, s)\n",
    "\n",
    "def augment_clip(frames):  # frames: [T, C, H, W]\n",
    "    (i, j, h, w), do_hflip, (fn_idx, b, c, s) = _sample_clip_params(frames[0])\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=InterpolationMode.BICUBIC)\n",
    "        if do_hflip: img = TF.hflip(img)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0: img = TF.adjust_brightness(img, b)\n",
    "            elif fn_id == 1: img = TF.adjust_contrast(img, c)\n",
    "            elif fn_id == 2: img = TF.adjust_saturation(img, s)\n",
    "            elif fn_id == 3: pass\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = TF.center_crop(img, IMG_SIZE)\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N: return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path, num_clips: int, train=True):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    if L == 0:\n",
    "        print(f\"[ê²½ê³ ] í”„ë ˆì„ì´ 0ì¸ ë¹„ë””ì˜¤: {path}\")\n",
    "        # ë¹ˆ í…ì„œ ë°˜í™˜ (í˜¹ì€ raise Exception)\n",
    "        return [torch.zeros((3, NUM_FRAMES, IMG_SIZE, IMG_SIZE), dtype=torch.float32) for _ in range(num_clips)]\n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous()\n",
    "        proc = augment_clip(clip) if train else eval_clip(clip)\n",
    "        clips.append(proc.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "\n",
    "# ======================= Dataset / Loader =======================\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance=\"none\", train=True, verbose=True):\n",
    "        self.train = train\n",
    "        true_samples, false_samples = [], []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            mp4_dir = (root / sub / \"crop_video\")\n",
    "            if mp4_dir.exists():\n",
    "                for p in mp4_dir.glob(\"*.mp4\"):\n",
    "                    (true_samples if label==1 else false_samples).append((p, label))\n",
    "\n",
    "        if balance == \"oversample\":\n",
    "            n_true, n_false = len(true_samples), len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor, remainder = n_true // n_false, n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        if verbose:\n",
    "            print(f\"âœ… {len(self.samples)} samples found in {root} (balance='{balance}')\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        num_clips = TRAIN_CLIPS_PER_VIDEO if self.train else EVAL_CLIPS_PER_VIDEO\n",
    "        clips = load_clip(path, num_clips=num_clips, train=self.train)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "def filesystem_class_weights(train_root: Path, num_classes=2):\n",
    "    true_cnt  = len(list((train_root/\"balanced_true\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    false_cnt = len(list((train_root/\"false\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    cnts = {1: true_cnt, 0: false_cnt}\n",
    "    total = sum(cnts.values())\n",
    "    if total == 0: return torch.ones(num_classes, dtype=torch.float32)\n",
    "    w = [total / max(1, cnts.get(c, 0)) for c in range(num_classes)]\n",
    "    s = sum(w); w = [wi * (num_classes / s) for wi in w]\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# oversample + class_weight ë™ì‹œ ì‚¬ìš© ê²½ê³  (ê³¼ë²Œì  ìœ„í—˜)\n",
    "if BALANCE_METHOD == \"oversample\":\n",
    "    print(\"âš ï¸  í˜„ì¬ oversample ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. val/testëŠ” í•­ìƒ balance='none'ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "elif BALANCE_METHOD == \"class_weight\":\n",
    "    print(\"â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 1) ì „ì²´ íŠ¸ë ˆì¸ ì„¸íŠ¸(ì¦ê°•/ì˜¤ë²„ìƒ˜í”Œ ì˜µì…˜ í¬í•¨)\n",
    "train_full = GolfSwingDataset(\n",
    "    TRAIN_ROOT,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\"),\n",
    "    train=True,\n",
    "    verbose=not CLEAN_OUTPUT\n",
    ")\n",
    "\n",
    "# 2) ì¸ë±ìŠ¤ ë¶„í• ë§Œ ì–»ê¸°\n",
    "val_len = max(1, int(len(train_full) * VAL_SPLIT))\n",
    "train_len = len(train_full) - val_len\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(train_full, [train_len, val_len], generator=g)\n",
    "\n",
    "# 3) ë™ì¼í•œ ìƒ˜í”Œ ëª©ë¡ì„ ê³µìœ í•˜ë˜, ì„œë¡œ ë‹¤ë¥¸ 'train í”Œë˜ê·¸'ë¥¼ ê°–ëŠ” ë³„ë„ Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "def clone_dataset_with_indices(src_ds: GolfSwingDataset, indices, train_flag: bool, balance: str):\n",
    "    ds = GolfSwingDataset(TRAIN_ROOT, balance=balance, train=train_flag, verbose=False)\n",
    "    ds.samples = [src_ds.samples[i] for i in indices]   # ì›ë³¸ì—ì„œ ì„ íƒëœ ìƒ˜í”Œë§Œ ë³µì‚¬\n",
    "    return ds\n",
    "\n",
    "train_dataset = clone_dataset_with_indices(\n",
    "    train_full, train_subset.indices, train_flag=True,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\")\n",
    ")\n",
    "val_dataset   = clone_dataset_with_indices(\n",
    "    train_full, val_subset.indices,   train_flag=False, balance=\"none\"\n",
    ")\n",
    "\n",
    "# 4) í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” ì›ë˜ëŒ€ë¡œ (í‰ê°€ ì „ìš©ì´ë¯€ë¡œ train=False)\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=\"none\", train=False, verbose=not CLEAN_OUTPUT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# ======================= Model / Optim =======================\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "base_model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES, num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PTH)\n",
    ").to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=DROPOUT).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model, train_blocks: int):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    if train_blocks == -1:\n",
    "        for p in base.parameters(): p.requires_grad = True\n",
    "        return [p for p in base.parameters() if p.requires_grad]\n",
    "    # Freeze all\n",
    "    for p in base.parameters(): p.requires_grad = False\n",
    "    # Unfreeze head\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    if head is not None:\n",
    "        for p in head.parameters(): p.requires_grad = True\n",
    "    # Unfreeze last N blocks\n",
    "    blocks = getattr(inner, \"blocks\", None)\n",
    "    if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "        total = len(blocks); start = max(0, total - train_blocks)\n",
    "        for b in blocks[start:]:\n",
    "            for p in b.parameters(): p.requires_grad = True\n",
    "    return [p for p in base.parameters() if p.requires_grad]\n",
    "\n",
    "def build_optimizer(model, train_blocks: int, base_lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    head_params = list(head.parameters()) if head is not None else []\n",
    "\n",
    "    if train_blocks == -1:\n",
    "        head_set = set(head_params)\n",
    "        backbone_params = [p for p in base.parameters() if p.requires_grad and p not in head_set]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,     \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * 0.5, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    else:\n",
    "        blocks = getattr(inner, \"blocks\", None)\n",
    "        block_params = []\n",
    "        if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "            total = len(blocks); start = max(0, total - train_blocks)\n",
    "            for b in blocks[start:]:\n",
    "                block_params += [p for p in b.parameters() if p.requires_grad]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,  \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": block_params, \"lr\": base_lr * 1.0, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(param_groups)\n",
    "    # === LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ê·¸ë£¹ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë„ë¡ initial_lr ì €ì¥ ===\n",
    "    for g in opt.param_groups:\n",
    "        g[\"initial_lr\"] = g[\"lr\"]\n",
    "    return opt\n",
    "\n",
    "trainable_params = get_trainable_params(model, TRAIN_BLOCK)\n",
    "optimizer = build_optimizer(model, TRAIN_BLOCK, base_lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class weights (ì˜µì…˜)\n",
    "if BALANCE_METHOD == \"class_weight\":\n",
    "    class_w = filesystem_class_weights(TRAIN_ROOT).to(DEVICE)\n",
    "else:\n",
    "    class_w = None\n",
    "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# AMP & LR schedule â€” ê·¸ë£¹ ë¹„ìœ¨ ìœ ì§€í˜• ìŠ¤ì¼€ì¼ íŒ©í„°\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def get_lr_factor(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    t = epoch - WARMUP_EPOCHS\n",
    "    T_total = max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1 + np.cos(np.pi * t / T_total))\n",
    "\n",
    "def set_lr_with_factor(optimr, factor):\n",
    "    for g in optimr.param_groups:\n",
    "        g['lr'] = g['initial_lr'] * factor\n",
    "\n",
    "# ======================= Train / Eval =======================\n",
    "def video_level_from_logits(logits, B, clips_per_video):\n",
    "    return logits.view(B, clips_per_video, -1).mean(dim=1)\n",
    "\n",
    "# ======================= Config (í•œ ê³³ì—ì„œ ì „ë¶€ ê´€ë¦¬) =======================\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings, random, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Paths\n",
    "TRAIN_ROOT      = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT       = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH    = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "EPOCHS_DIR      = Path(\"epochs\"); EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "BEST_CKPT       = Path(\"best_timesformer.pth\")\n",
    "\n",
    "# ---- Data / Model\n",
    "IMG_SIZE               = 224\n",
    "NUM_FRAMES             = 16\n",
    "TRAIN_CLIPS_PER_VIDEO  = 1\n",
    "EVAL_CLIPS_PER_VIDEO   = 8   # í•„ìš”ì‹œ 5~10ìœ¼ë¡œ ì˜¬ë¦¬ë©´ ë³€ë™ì„±â†“(ì‹œê°„ ë¹„ìš©â†‘)\n",
    "\n",
    "# ---- Train hyperparams\n",
    "BATCH_SIZE     = 2\n",
    "WORKERS        = 0          # Windowsë©´ 2~4 ê¶Œì¥, Linuxë©´ ë” ë†’ì—¬ë„ OK\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 0.01\n",
    "DROPOUT        = 0.2\n",
    "TRAIN_BLOCK    = 2           # -1ì´ë©´ ì „ì¸µ í•™ìŠµ\n",
    "VAL_SPLIT      = 0.10        # train -> train/val\n",
    "BALANCE_METHOD = \"class_weight\"   # 'none' | 'oversample' | 'class_weight'\n",
    "EPOCHS         = 20\n",
    "WARMUP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "EARLY_STOP     = 5\n",
    "LABEL_SMOOTH   = 0.10\n",
    "SEED           = 42\n",
    "\n",
    "# (ì„ íƒ) grad accumulation ì¶”ê°€ ì‹œ\n",
    "GRAD_ACCUM_STEPS = 2     # ìœ íš¨ ë°°ì¹˜ = BATCH_SIZE * GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# ---- Logs / UI\n",
    "CLEAN_OUTPUT         = True\n",
    "SHOW_TRAIN_BAR       = True\n",
    "SHOW_EVAL_BAR        = True\n",
    "PLOT_CM_EVERY_EPOCH  = True\n",
    "PRINT_EPOCH_SAVE     = True\n",
    "\n",
    "# ======================= Imports (í•œ ë²ˆë§Œ) =======================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ======================= Repro / Device =======================\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======================= Augment / Sampling =======================\n",
    "def _sample_clip_params(first_frame_tensor):\n",
    "    scale = (0.8, 1.0)\n",
    "    i, j, h, w = T.RandomResizedCrop.get_params(first_frame_tensor, scale=scale, ratio=(3/4, 4/3))\n",
    "    do_hflip = random.random() < 0.5\n",
    "    cj = T.ColorJitter(0.2, 0.2, 0.2, 0.0)\n",
    "    fn_idx, b, c, s, h2 = T.ColorJitter.get_params(cj.brightness, cj.contrast, cj.saturation, cj.hue)\n",
    "    return (i, j, h, w), do_hflip, (fn_idx, b, c, s)\n",
    "\n",
    "def augment_clip(frames):  # frames: [T, C, H, W]\n",
    "    (i, j, h, w), do_hflip, (fn_idx, b, c, s) = _sample_clip_params(frames[0])\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=InterpolationMode.BICUBIC)\n",
    "        if do_hflip: img = TF.hflip(img)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0: img = TF.adjust_brightness(img, b)\n",
    "            elif fn_id == 1: img = TF.adjust_contrast(img, c)\n",
    "            elif fn_id == 2: img = TF.adjust_saturation(img, s)\n",
    "            elif fn_id == 3: pass\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = TF.center_crop(img, IMG_SIZE)\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N: return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path, num_clips: int, train=True):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    if L == 0:\n",
    "        print(f\"[ê²½ê³ ] í”„ë ˆì„ì´ 0ì¸ ë¹„ë””ì˜¤: {path}\")\n",
    "        # ë¹ˆ í…ì„œ ë°˜í™˜ (í˜¹ì€ raise Exception)\n",
    "        return [torch.zeros((3, NUM_FRAMES, IMG_SIZE, IMG_SIZE), dtype=torch.float32) for _ in range(num_clips)]\n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous()\n",
    "        proc = augment_clip(clip) if train else eval_clip(clip)\n",
    "        clips.append(proc.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "\n",
    "# ======================= Dataset / Loader =======================\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance=\"none\", train=True, verbose=True):\n",
    "        self.train = train\n",
    "        true_samples, false_samples = [], []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            mp4_dir = (root / sub / \"crop_video\")\n",
    "            if mp4_dir.exists():\n",
    "                for p in mp4_dir.glob(\"*.mp4\"):\n",
    "                    (true_samples if label==1 else false_samples).append((p, label))\n",
    "\n",
    "        if balance == \"oversample\":\n",
    "            n_true, n_false = len(true_samples), len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor, remainder = n_true // n_false, n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        if verbose:\n",
    "            print(f\"âœ… {len(self.samples)} samples found in {root} (balance='{balance}')\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        num_clips = TRAIN_CLIPS_PER_VIDEO if self.train else EVAL_CLIPS_PER_VIDEO\n",
    "        clips = load_clip(path, num_clips=num_clips, train=self.train)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "def filesystem_class_weights(train_root: Path, num_classes=2):\n",
    "    true_cnt  = len(list((train_root/\"balanced_true\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    false_cnt = len(list((train_root/\"false\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    cnts = {1: true_cnt, 0: false_cnt}\n",
    "    total = sum(cnts.values())\n",
    "    if total == 0: return torch.ones(num_classes, dtype=torch.float32)\n",
    "    w = [total / max(1, cnts.get(c, 0)) for c in range(num_classes)]\n",
    "    s = sum(w); w = [wi * (num_classes / s) for wi in w]\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# oversample + class_weight ë™ì‹œ ì‚¬ìš© ê²½ê³  (ê³¼ë²Œì  ìœ„í—˜)\n",
    "if BALANCE_METHOD == \"oversample\":\n",
    "    print(\"âš ï¸  í˜„ì¬ oversample ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. val/testëŠ” í•­ìƒ balance='none'ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "elif BALANCE_METHOD == \"class_weight\":\n",
    "    print(\"â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 1) ì „ì²´ íŠ¸ë ˆì¸ ì„¸íŠ¸(ì¦ê°•/ì˜¤ë²„ìƒ˜í”Œ ì˜µì…˜ í¬í•¨)\n",
    "train_full = GolfSwingDataset(\n",
    "    TRAIN_ROOT,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\"),\n",
    "    train=True,\n",
    "    verbose=not CLEAN_OUTPUT\n",
    ")\n",
    "\n",
    "# 2) ì¸ë±ìŠ¤ ë¶„í• ë§Œ ì–»ê¸°\n",
    "val_len = max(1, int(len(train_full) * VAL_SPLIT))\n",
    "train_len = len(train_full) - val_len\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(train_full, [train_len, val_len], generator=g)\n",
    "\n",
    "# 3) ë™ì¼í•œ ìƒ˜í”Œ ëª©ë¡ì„ ê³µìœ í•˜ë˜, ì„œë¡œ ë‹¤ë¥¸ 'train í”Œë˜ê·¸'ë¥¼ ê°–ëŠ” ë³„ë„ Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "def clone_dataset_with_indices(src_ds: GolfSwingDataset, indices, train_flag: bool, balance: str):\n",
    "    ds = GolfSwingDataset(TRAIN_ROOT, balance=balance, train=train_flag, verbose=False)\n",
    "    ds.samples = [src_ds.samples[i] for i in indices]   # ì›ë³¸ì—ì„œ ì„ íƒëœ ìƒ˜í”Œë§Œ ë³µì‚¬\n",
    "    return ds\n",
    "\n",
    "train_dataset = clone_dataset_with_indices(\n",
    "    train_full, train_subset.indices, train_flag=True,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\")\n",
    ")\n",
    "val_dataset   = clone_dataset_with_indices(\n",
    "    train_full, val_subset.indices,   train_flag=False, balance=\"none\"\n",
    ")\n",
    "\n",
    "# 4) í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” ì›ë˜ëŒ€ë¡œ (í‰ê°€ ì „ìš©ì´ë¯€ë¡œ train=False)\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=\"none\", train=False, verbose=not CLEAN_OUTPUT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# ======================= Model / Optim =======================\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "base_model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES, num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PTH)\n",
    ").to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=DROPOUT).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model, train_blocks: int):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    if train_blocks == -1:\n",
    "        for p in base.parameters(): p.requires_grad = True\n",
    "        return [p for p in base.parameters() if p.requires_grad]\n",
    "    # Freeze all\n",
    "    for p in base.parameters(): p.requires_grad = False\n",
    "    # Unfreeze head\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    if head is not None:\n",
    "        for p in head.parameters(): p.requires_grad = True\n",
    "    # Unfreeze last N blocks\n",
    "    blocks = getattr(inner, \"blocks\", None)\n",
    "    if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "        total = len(blocks); start = max(0, total - train_blocks)\n",
    "        for b in blocks[start:]:\n",
    "            for p in b.parameters(): p.requires_grad = True\n",
    "    return [p for p in base.parameters() if p.requires_grad]\n",
    "\n",
    "def build_optimizer(model, train_blocks: int, base_lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    head_params = list(head.parameters()) if head is not None else []\n",
    "\n",
    "    if train_blocks == -1:\n",
    "        head_set = set(head_params)\n",
    "        backbone_params = [p for p in base.parameters() if p.requires_grad and p not in head_set]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,     \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * 0.5, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    else:\n",
    "        blocks = getattr(inner, \"blocks\", None)\n",
    "        block_params = []\n",
    "        if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "            total = len(blocks); start = max(0, total - train_blocks)\n",
    "            for b in blocks[start:]:\n",
    "                block_params += [p for p in b.parameters() if p.requires_grad]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,  \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": block_params, \"lr\": base_lr * 1.0, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(param_groups)\n",
    "    # === LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ê·¸ë£¹ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë„ë¡ initial_lr ì €ì¥ ===\n",
    "    for g in opt.param_groups:\n",
    "        g[\"initial_lr\"] = g[\"lr\"]\n",
    "    return opt\n",
    "\n",
    "trainable_params = get_trainable_params(model, TRAIN_BLOCK)\n",
    "optimizer = build_optimizer(model, TRAIN_BLOCK, base_lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class weights (ì˜µì…˜)\n",
    "if BALANCE_METHOD == \"class_weight\":\n",
    "    class_w = filesystem_class_weights(TRAIN_ROOT).to(DEVICE)\n",
    "else:\n",
    "    class_w = None\n",
    "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# AMP & LR schedule â€” ê·¸ë£¹ ë¹„ìœ¨ ìœ ì§€í˜• ìŠ¤ì¼€ì¼ íŒ©í„°\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def get_lr_factor(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    t = epoch - WARMUP_EPOCHS\n",
    "    T_total = max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1 + np.cos(np.pi * t / T_total))\n",
    "\n",
    "def set_lr_with_factor(optimr, factor):\n",
    "    for g in optimr.param_groups:\n",
    "        g['lr'] = g['initial_lr'] * factor\n",
    "\n",
    "# ======================= Train / Eval =======================\n",
    "def video_level_from_logits(logits, B, clips_per_video):\n",
    "    return logits.view(B, clips_per_video, -1).mean(dim=1)\n",
    "\n",
    "train_logs = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": []}\n",
    "val_acc_per_epoch, best_val_acc, patience = [], -1.0, 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    lr_factor = get_lr_factor(epoch)\n",
    "    set_lr_with_factor(optimizer, lr_factor)\n",
    "\n",
    "    total_videos = correct_videos = 0\n",
    "    running_loss, running_steps = 0.0, 0\n",
    "\n",
    "    bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\",\n",
    "            ncols=80, leave=False, disable=not SHOW_TRAIN_BAR)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)  # âœ… ë£¨í”„ ì‹œì‘ ì „ì— 1ë²ˆë§Œ\n",
    "\n",
    "    for step, (clips, label) in enumerate(bar):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)               # [B*C, C,T,H,W]\n",
    "        labs = label.repeat_interleave(TRAIN_CLIPS_PER_VIDEO).to(DEVICE) # ë¼ë²¨ í™•ì¥\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)                                            # [B*C, 2]\n",
    "            loss = criterion(outs, labs)\n",
    "\n",
    "        # âœ… ëˆ„ì : ìŠ¤ì¼€ì¼í•œ lossë¥¼ ë‚˜ëˆ ì„œ backward\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "\n",
    "        # ë¡œê¹…ì€ ì›ë˜ loss ê¸°ì¤€ìœ¼ë¡œ\n",
    "        running_loss += loss.item()\n",
    "        running_steps += 1\n",
    "\n",
    "        # ë¹„ë””ì˜¤ ë‹¨ìœ„ ì •í™•ë„ ê°±ì‹ (ë¡œì§“ í‰ê· )\n",
    "        with torch.no_grad():\n",
    "            vid_logits = video_level_from_logits(outs, B, TRAIN_CLIPS_PER_VIDEO)\n",
    "            preds = vid_logits.argmax(1).cpu()\n",
    "            correct_videos += (preds == label).sum().item()\n",
    "            total_videos   += B\n",
    "\n",
    "        # âœ… ëˆ„ì  stepë§ˆë‹¤ë§Œ step/zero_grad\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # AMP ì‚¬ìš© ì‹œ, clip ì „ì— unscale ê¶Œì¥\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if SHOW_TRAIN_BAR:\n",
    "            bar.set_postfix({\n",
    "                \"Loss\": f\"{running_loss/max(1,running_steps):.4f}\",\n",
    "                \"Acc\":  f\"{correct_videos/max(1,total_videos):.2%}\"\n",
    "            })\n",
    "\n",
    "    # âœ… ë§ˆì§€ë§‰ì— ë‚¨ì€ ê·¸ë¼ë””ì–¸íŠ¸ ì²˜ë¦¬(ë¯¸ì™„ ë°°ì¹˜ê°€ ìˆì„ ë•Œ)\n",
    "    remainder = (step + 1) % GRAD_ACCUM_STEPS\n",
    "    if remainder != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    train_acc = correct_videos / max(1, total_videos)\n",
    "    avg_loss  = running_loss / max(1, running_steps)\n",
    "    train_logs[\"train_acc\"].append(train_acc); train_logs[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval(); all_preds, all_labels = [], []\n",
    "    val_total = val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        vbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\",\n",
    "                    ncols=80, leave=False, disable=not SHOW_EVAL_BAR)\n",
    "        for clips, label in vbar:\n",
    "            B = clips.shape[0]\n",
    "            vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "            with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "                outs = model(vids)\n",
    "                vid_logits = video_level_from_logits(outs, B, EVAL_CLIPS_PER_VIDEO)\n",
    "                pred = vid_logits.argmax(1).cpu()\n",
    "            val_total += B; val_correct += (pred == label).sum().item()\n",
    "            if SHOW_EVAL_BAR: vbar.set_postfix({\"Acc\": f\"{val_correct/max(1,val_total):.2%}\"})\n",
    "            all_preds.extend(pred.numpy()); all_labels.extend(label.numpy())\n",
    "\n",
    "    val_acc = val_correct / max(1, val_total)\n",
    "    train_logs[\"val_acc\"].append(val_acc); val_acc_per_epoch.append(val_acc)\n",
    "\n",
    "    if PLOT_CM_EVERY_EPOCH:\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "        disp.plot(cmap=\"Blues\"); plt.title(f\"Val Confusion Matrix (Epoch {epoch+1})\"); plt.show()\n",
    "\n",
    "    # ---- Early stop / Save best (val ê¸°ì¤€)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc, patience = val_acc, 0\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict(),\n",
    "                    \"opt\": optimizer.state_dict(), \"metrics\": {\"val_acc\": val_acc}}, BEST_CKPT)\n",
    "        if not CLEAN_OUTPUT or PRINT_EPOCH_SAVE:\n",
    "            print(f\"  âœ… Best updated: val_acc={val_acc:.3%} -> saved to {BEST_CKPT}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= EARLY_STOP:\n",
    "            print(\"â›³ Early stopping triggered.\"); break\n",
    "\n",
    "    if CLEAN_OUTPUT:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | train_acc={train_acc:.3%} loss={avg_loss:.4f} \"\n",
    "              f\"| val_acc={val_acc:.3%} | best={best_val_acc:.3%}\")\n",
    "\n",
    "    with open(\"train_logs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_logs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1547d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 257/257 [08:59<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test Accuracy: 72.515%\n",
      "ğŸ” Balanced Acc : 51.874%\n",
      "ğŸ” Macro F1     : 46.983%\n",
      "ğŸ” ROC-AUC      : 0.540\n",
      "ğŸ” PR-AUC(AP)   : 0.742\n",
      "\n",
      "[Per-class]\n",
      " false(0) -> P:0.533 R:0.056 F1:0.102 (n=142)\n",
      " true (1) -> P:0.731 R:0.981 F1:0.838 (n=371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================= Final Test (ì •í™•ë„ + ë¶ˆê· í˜• ì§€í‘œ) =======================\n",
    "# í•„ìš”ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ (í•™ìŠµ í›„ ë°”ë¡œ í…ŒìŠ¤íŠ¸í•  ê²½ìš° ìƒëµ ê°€ëŠ¥)\n",
    "model.load_state_dict(torch.load(BEST_CKPT)[\"model\"])\n",
    "\n",
    "model.eval(); all_preds, all_labels, all_scores = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(test_loader, desc=\"[Test]\", ncols=80, disable=not SHOW_EVAL_BAR):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)  # [B*C, 2]\n",
    "            # ë™ì ìœ¼ë¡œ clips_per_video ê³„ì‚°\n",
    "            clips_per_video = outs.shape[0] // B\n",
    "            vid_logits = outs.view(B, clips_per_video, -1).mean(dim=1)  # [B,2]\n",
    "            probs = F.softmax(vid_logits, dim=1)  # í™•ë¥  ìŠ¤ì½”ì–´\n",
    "            pred = vid_logits.argmax(1).cpu()\n",
    "\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "        all_scores.extend(probs[:, 1].cpu().numpy())  # ì–‘ì„±( true ) í™•ë¥ , í•„ìš” ì‹œ falseë©´ [:,0]\n",
    "\n",
    "# ì›ë˜ accuracy\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\nâœ… Test Accuracy: {test_acc:.3%}\")\n",
    "\n",
    "# ë¶ˆê· í˜• ì¹œí™” ì§€í‘œ\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "# ROC-AUC: ì–‘ì„± í´ë˜ìŠ¤ë¥¼ 1ë¡œ ê°€ì •\n",
    "try:\n",
    "    roc_auc = roc_auc_score(all_labels, np.array(all_scores))\n",
    "except ValueError:\n",
    "    roc_auc = float('nan')\n",
    "# PR-AUC (ì–‘ì„± ê¸°ì¤€)\n",
    "ap = average_precision_score(all_labels, np.array(all_scores))\n",
    "\n",
    "print(f\"ğŸ” Balanced Acc : {bal_acc:.3%}\")\n",
    "print(f\"ğŸ” Macro F1     : {macro_f1:.3%}\")\n",
    "print(f\"ğŸ” ROC-AUC      : {roc_auc:.3f}\")\n",
    "print(f\"ğŸ” PR-AUC(AP)   : {ap:.3f}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ì •ë°€ë„/ì¬í˜„ìœ¨/F1\n",
    "prec, rec, f1, supp = precision_recall_fscore_support(all_labels, all_preds, labels=[0,1], zero_division=0)\n",
    "print(\"\\n[Per-class]\")\n",
    "print(f\" false(0) -> P:{prec[0]:.3f} R:{rec[0]:.3f} F1:{f1[0]:.3f} (n={supp[0]})\")\n",
    "print(f\" true (1) -> P:{prec[1]:.3f} R:{rec[1]:.3f} F1:{f1[1]:.3f} (n={supp[1]})\")\n",
    "\n",
    "# ì •ê·œí™”ëœ í˜¼ë™í–‰ë ¬(ë¹„ìœ¨)\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1], normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\"); plt.title(\"Test CM (Normalized)\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432714d",
   "metadata": {},
   "source": [
    "# true, false 1ëŒ€1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# ======================= 1:1 (false:true) Test Evaluation =======================\n",
    "model.load_state_dict(torch.load(BEST_CKPT)[\"model\"])\n",
    "\n",
    "# 1. test_datasetì—ì„œ false/true ì¸ë±ìŠ¤ ë¶„ë¦¬\n",
    "false_indices = [i for i, (p, l) in enumerate(test_dataset.samples) if l == 0]\n",
    "true_indices  = [i for i, (p, l) in enumerate(test_dataset.samples) if l == 1]\n",
    "\n",
    "# 2. false ê°œìˆ˜ë§Œí¼ trueì—ì„œ ëœë¤ ì¶”ì¶œ\n",
    "random.seed(SEED)\n",
    "n_false = len(false_indices)\n",
    "sampled_true_indices = random.sample(true_indices, min(n_false, len(true_indices)))\n",
    "\n",
    "# 3. í•©ì³ì„œ 1:1 ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì…”í”Œ\n",
    "balanced_indices = false_indices + sampled_true_indices\n",
    "random.shuffle(balanced_indices)\n",
    "\n",
    "# 4. Subset DataLoader ìƒì„±\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "balanced_subset = Subset(test_dataset, balanced_indices)\n",
    "balanced_loader = DataLoader(balanced_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# 5. í‰ê°€\n",
    "model.eval(); all_preds, all_labels, all_scores = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(balanced_loader, desc=\"[Test 1:1]\", ncols=80, disable=not SHOW_EVAL_BAR):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)\n",
    "            clips_per_video = outs.shape[0] // B\n",
    "            vid_logits = outs.view(B, clips_per_video, -1).mean(dim=1)\n",
    "            probs = F.softmax(vid_logits, dim=1)\n",
    "            pred = vid_logits.argmax(1).cpu()\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "        all_scores.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "# 6. ì§€í‘œ ì¶œë ¥\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "try:\n",
    "    roc_auc = roc_auc_score(all_labels, np.array(all_scores))\n",
    "except ValueError:\n",
    "    roc_auc = float('nan')\n",
    "ap = average_precision_score(all_labels, np.array(all_scores))\n",
    "\n",
    "print(f\"\\nâœ… [1:1] Test Accuracy: {test_acc:.3%}\")\n",
    "print(f\"ğŸ” Balanced Acc : {bal_acc:.3%}\")\n",
    "print(f\"ğŸ” Macro F1     : {macro_f1:.3%}\")\n",
    "print(f\"ğŸ” ROC-AUC      : {roc_auc:.3f}\")\n",
    "print(f\"ğŸ” PR-AUC(AP)   : {ap:.3f}\")\n",
    "\n",
    "prec, rec, f1, supp = precision_recall_fscore_support(all_labels, all_preds, labels=[0,1], zero_division=0)\n",
    "print(\"\\n[Per-class]\")\n",
    "print(f\" false(0) -> P:{prec[0]:.3f} R:{rec[0]:.3f} F1:{f1[0]:.3f} (n={supp[0]})\")\n",
    "print(f\" true (1) -> P:{prec[1]:.3f} R:{rec[1]:.3f} F1:{f1[1]:.3f} (n={supp[1]})\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1], normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\"); plt.title(\"Test 1:1 CM (Normalized)\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
