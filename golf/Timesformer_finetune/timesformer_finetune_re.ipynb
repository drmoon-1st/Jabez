{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7802a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\n",
      "â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 653\u001b[39m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=(DEVICE==\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    652\u001b[39m     outs = model(vids)                                            \u001b[38;5;66;03m# [B*C, 2]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;66;03m# âœ… ëˆ„ì : ìŠ¤ì¼€ì¼í•œ lossë¥¼ ë‚˜ëˆ ì„œ backward\u001b[39;00m\n\u001b[32m    656\u001b[39m scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qqppq\\anaconda3\\envs\\timesformer\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ======================= Config (í•œ ê³³ì—ì„œ ì „ë¶€ ê´€ë¦¬) =======================\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings, random, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Paths\n",
    "TRAIN_ROOT      = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT       = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH    = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "EPOCHS_DIR      = Path(\"epochs\"); EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "BEST_CKPT       = Path(\"best_timesformer.pth\")\n",
    "\n",
    "# ---- Data / Model\n",
    "IMG_SIZE               = 224\n",
    "NUM_FRAMES             = 16\n",
    "TRAIN_CLIPS_PER_VIDEO  = 1\n",
    "EVAL_CLIPS_PER_VIDEO   = 8   # í•„ìš”ì‹œ 5~10ìœ¼ë¡œ ì˜¬ë¦¬ë©´ ë³€ë™ì„±â†“(ì‹œê°„ ë¹„ìš©â†‘)\n",
    "\n",
    "# ---- Train hyperparams\n",
    "BATCH_SIZE     = 2\n",
    "WORKERS        = 0          # Windowsë©´ 2~4 ê¶Œì¥, Linuxë©´ ë” ë†’ì—¬ë„ OK\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 0.01\n",
    "DROPOUT        = 0.2\n",
    "TRAIN_BLOCK    = 2           # -1ì´ë©´ ì „ì¸µ í•™ìŠµ\n",
    "VAL_SPLIT      = 0.10        # train -> train/val\n",
    "BALANCE_METHOD = \"class_weight\"   # 'none' | 'oversample' | 'class_weight'\n",
    "EPOCHS         = 20\n",
    "WARMUP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "EARLY_STOP     = 5\n",
    "LABEL_SMOOTH   = 0.10\n",
    "SEED           = 42\n",
    "\n",
    "# (ì„ íƒ) grad accumulation ì¶”ê°€ ì‹œ\n",
    "GRAD_ACCUM_STEPS = 2     # ìœ íš¨ ë°°ì¹˜ = BATCH_SIZE * GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# ---- Logs / UI\n",
    "CLEAN_OUTPUT         = True\n",
    "SHOW_TRAIN_BAR       = True\n",
    "SHOW_EVAL_BAR        = True\n",
    "PLOT_CM_EVERY_EPOCH  = True\n",
    "PRINT_EPOCH_SAVE     = True\n",
    "\n",
    "# ======================= Imports (í•œ ë²ˆë§Œ) =======================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ======================= Repro / Device =======================\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======================= Augment / Sampling =======================\n",
    "def _sample_clip_params(first_frame_tensor):\n",
    "    scale = (0.8, 1.0)\n",
    "    i, j, h, w = T.RandomResizedCrop.get_params(first_frame_tensor, scale=scale, ratio=(3/4, 4/3))\n",
    "    do_hflip = random.random() < 0.5\n",
    "    cj = T.ColorJitter(0.2, 0.2, 0.2, 0.0)\n",
    "    fn_idx, b, c, s, h2 = T.ColorJitter.get_params(cj.brightness, cj.contrast, cj.saturation, cj.hue)\n",
    "    return (i, j, h, w), do_hflip, (fn_idx, b, c, s)\n",
    "\n",
    "def augment_clip(frames):  # frames: [T, C, H, W]\n",
    "    (i, j, h, w), do_hflip, (fn_idx, b, c, s) = _sample_clip_params(frames[0])\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=InterpolationMode.BICUBIC)\n",
    "        if do_hflip: img = TF.hflip(img)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0: img = TF.adjust_brightness(img, b)\n",
    "            elif fn_id == 1: img = TF.adjust_contrast(img, c)\n",
    "            elif fn_id == 2: img = TF.adjust_saturation(img, s)\n",
    "            elif fn_id == 3: pass\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = TF.center_crop(img, IMG_SIZE)\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N: return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path, num_clips: int, train=True):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    if L == 0:\n",
    "        print(f\"[ê²½ê³ ] í”„ë ˆì„ì´ 0ì¸ ë¹„ë””ì˜¤: {path}\")\n",
    "        # ë¹ˆ í…ì„œ ë°˜í™˜ (í˜¹ì€ raise Exception)\n",
    "        return [torch.zeros((3, NUM_FRAMES, IMG_SIZE, IMG_SIZE), dtype=torch.float32) for _ in range(num_clips)]\n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous()\n",
    "        proc = augment_clip(clip) if train else eval_clip(clip)\n",
    "        clips.append(proc.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "\n",
    "# ======================= Dataset / Loader =======================\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance=\"none\", train=True, verbose=True):\n",
    "        self.train = train\n",
    "        true_samples, false_samples = [], []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            mp4_dir = (root / sub / \"crop_video\")\n",
    "            if mp4_dir.exists():\n",
    "                for p in mp4_dir.glob(\"*.mp4\"):\n",
    "                    (true_samples if label==1 else false_samples).append((p, label))\n",
    "\n",
    "        if balance == \"oversample\":\n",
    "            n_true, n_false = len(true_samples), len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor, remainder = n_true // n_false, n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        if verbose:\n",
    "            print(f\"âœ… {len(self.samples)} samples found in {root} (balance='{balance}')\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        num_clips = TRAIN_CLIPS_PER_VIDEO if self.train else EVAL_CLIPS_PER_VIDEO\n",
    "        clips = load_clip(path, num_clips=num_clips, train=self.train)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "def filesystem_class_weights(train_root: Path, num_classes=2):\n",
    "    true_cnt  = len(list((train_root/\"balanced_true\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    false_cnt = len(list((train_root/\"false\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    cnts = {1: true_cnt, 0: false_cnt}\n",
    "    total = sum(cnts.values())\n",
    "    if total == 0: return torch.ones(num_classes, dtype=torch.float32)\n",
    "    w = [total / max(1, cnts.get(c, 0)) for c in range(num_classes)]\n",
    "    s = sum(w); w = [wi * (num_classes / s) for wi in w]\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# oversample + class_weight ë™ì‹œ ì‚¬ìš© ê²½ê³  (ê³¼ë²Œì  ìœ„í—˜)\n",
    "if BALANCE_METHOD == \"oversample\":\n",
    "    print(\"âš ï¸  í˜„ì¬ oversample ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. val/testëŠ” í•­ìƒ balance='none'ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "elif BALANCE_METHOD == \"class_weight\":\n",
    "    print(\"â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 1) ì „ì²´ íŠ¸ë ˆì¸ ì„¸íŠ¸(ì¦ê°•/ì˜¤ë²„ìƒ˜í”Œ ì˜µì…˜ í¬í•¨)\n",
    "train_full = GolfSwingDataset(\n",
    "    TRAIN_ROOT,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\"),\n",
    "    train=True,\n",
    "    verbose=not CLEAN_OUTPUT\n",
    ")\n",
    "\n",
    "# 2) ì¸ë±ìŠ¤ ë¶„í• ë§Œ ì–»ê¸°\n",
    "val_len = max(1, int(len(train_full) * VAL_SPLIT))\n",
    "train_len = len(train_full) - val_len\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(train_full, [train_len, val_len], generator=g)\n",
    "\n",
    "# 3) ë™ì¼í•œ ìƒ˜í”Œ ëª©ë¡ì„ ê³µìœ í•˜ë˜, ì„œë¡œ ë‹¤ë¥¸ 'train í”Œë˜ê·¸'ë¥¼ ê°–ëŠ” ë³„ë„ Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "def clone_dataset_with_indices(src_ds: GolfSwingDataset, indices, train_flag: bool, balance: str):\n",
    "    ds = GolfSwingDataset(TRAIN_ROOT, balance=balance, train=train_flag, verbose=False)\n",
    "    ds.samples = [src_ds.samples[i] for i in indices]   # ì›ë³¸ì—ì„œ ì„ íƒëœ ìƒ˜í”Œë§Œ ë³µì‚¬\n",
    "    return ds\n",
    "\n",
    "train_dataset = clone_dataset_with_indices(\n",
    "    train_full, train_subset.indices, train_flag=True,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\")\n",
    ")\n",
    "val_dataset   = clone_dataset_with_indices(\n",
    "    train_full, val_subset.indices,   train_flag=False, balance=\"none\"\n",
    ")\n",
    "\n",
    "# 4) í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” ì›ë˜ëŒ€ë¡œ (í‰ê°€ ì „ìš©ì´ë¯€ë¡œ train=False)\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=\"none\", train=False, verbose=not CLEAN_OUTPUT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# ======================= Model / Optim =======================\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "base_model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES, num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PTH)\n",
    ").to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=DROPOUT).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model, train_blocks: int):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    if train_blocks == -1:\n",
    "        for p in base.parameters(): p.requires_grad = True\n",
    "        return [p for p in base.parameters() if p.requires_grad]\n",
    "    # Freeze all\n",
    "    for p in base.parameters(): p.requires_grad = False\n",
    "    # Unfreeze head\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    if head is not None:\n",
    "        for p in head.parameters(): p.requires_grad = True\n",
    "    # Unfreeze last N blocks\n",
    "    blocks = getattr(inner, \"blocks\", None)\n",
    "    if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "        total = len(blocks); start = max(0, total - train_blocks)\n",
    "        for b in blocks[start:]:\n",
    "            for p in b.parameters(): p.requires_grad = True\n",
    "    return [p for p in base.parameters() if p.requires_grad]\n",
    "\n",
    "def build_optimizer(model, train_blocks: int, base_lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    head_params = list(head.parameters()) if head is not None else []\n",
    "\n",
    "    if train_blocks == -1:\n",
    "        head_set = set(head_params)\n",
    "        backbone_params = [p for p in base.parameters() if p.requires_grad and p not in head_set]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,     \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * 0.5, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    else:\n",
    "        blocks = getattr(inner, \"blocks\", None)\n",
    "        block_params = []\n",
    "        if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "            total = len(blocks); start = max(0, total - train_blocks)\n",
    "            for b in blocks[start:]:\n",
    "                block_params += [p for p in b.parameters() if p.requires_grad]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,  \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": block_params, \"lr\": base_lr * 1.0, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(param_groups)\n",
    "    # === LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ê·¸ë£¹ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë„ë¡ initial_lr ì €ì¥ ===\n",
    "    for g in opt.param_groups:\n",
    "        g[\"initial_lr\"] = g[\"lr\"]\n",
    "    return opt\n",
    "\n",
    "trainable_params = get_trainable_params(model, TRAIN_BLOCK)\n",
    "optimizer = build_optimizer(model, TRAIN_BLOCK, base_lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class weights (ì˜µì…˜)\n",
    "if BALANCE_METHOD == \"class_weight\":\n",
    "    class_w = filesystem_class_weights(TRAIN_ROOT).to(DEVICE)\n",
    "else:\n",
    "    class_w = None\n",
    "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# AMP & LR schedule â€” ê·¸ë£¹ ë¹„ìœ¨ ìœ ì§€í˜• ìŠ¤ì¼€ì¼ íŒ©í„°\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def get_lr_factor(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    t = epoch - WARMUP_EPOCHS\n",
    "    T_total = max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1 + np.cos(np.pi * t / T_total))\n",
    "\n",
    "def set_lr_with_factor(optimr, factor):\n",
    "    for g in optimr.param_groups:\n",
    "        g['lr'] = g['initial_lr'] * factor\n",
    "\n",
    "# ======================= Train / Eval =======================\n",
    "def video_level_from_logits(logits, B, clips_per_video):\n",
    "    return logits.view(B, clips_per_video, -1).mean(dim=1)\n",
    "\n",
    "# ======================= Config (í•œ ê³³ì—ì„œ ì „ë¶€ ê´€ë¦¬) =======================\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings, random, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Paths\n",
    "TRAIN_ROOT      = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT       = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH    = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_8x32_224_K600.pyth\")\n",
    "EPOCHS_DIR      = Path(\"epochs\"); EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "BEST_CKPT       = Path(\"best_timesformer.pth\")\n",
    "\n",
    "# ---- Data / Model\n",
    "IMG_SIZE               = 224\n",
    "NUM_FRAMES             = 16\n",
    "TRAIN_CLIPS_PER_VIDEO  = 1\n",
    "EVAL_CLIPS_PER_VIDEO   = 8   # í•„ìš”ì‹œ 5~10ìœ¼ë¡œ ì˜¬ë¦¬ë©´ ë³€ë™ì„±â†“(ì‹œê°„ ë¹„ìš©â†‘)\n",
    "\n",
    "# ---- Train hyperparams\n",
    "BATCH_SIZE     = 2\n",
    "WORKERS        = 0          # Windowsë©´ 2~4 ê¶Œì¥, Linuxë©´ ë” ë†’ì—¬ë„ OK\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 0.01\n",
    "DROPOUT        = 0.2\n",
    "TRAIN_BLOCK    = 2           # -1ì´ë©´ ì „ì¸µ í•™ìŠµ\n",
    "VAL_SPLIT      = 0.10        # train -> train/val\n",
    "BALANCE_METHOD = \"class_weight\"   # 'none' | 'oversample' | 'class_weight'\n",
    "EPOCHS         = 20\n",
    "WARMUP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "EARLY_STOP     = 5\n",
    "LABEL_SMOOTH   = 0.10\n",
    "SEED           = 42\n",
    "\n",
    "# (ì„ íƒ) grad accumulation ì¶”ê°€ ì‹œ\n",
    "GRAD_ACCUM_STEPS = 2     # ìœ íš¨ ë°°ì¹˜ = BATCH_SIZE * GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# ---- Logs / UI\n",
    "CLEAN_OUTPUT         = True\n",
    "SHOW_TRAIN_BAR       = True\n",
    "SHOW_EVAL_BAR        = True\n",
    "PLOT_CM_EVERY_EPOCH  = True\n",
    "PRINT_EPOCH_SAVE     = True\n",
    "\n",
    "# ======================= Imports (í•œ ë²ˆë§Œ) =======================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ======================= Repro / Device =======================\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======================= Augment / Sampling =======================\n",
    "def _sample_clip_params(first_frame_tensor):\n",
    "    scale = (0.8, 1.0)\n",
    "    i, j, h, w = T.RandomResizedCrop.get_params(first_frame_tensor, scale=scale, ratio=(3/4, 4/3))\n",
    "    do_hflip = random.random() < 0.5\n",
    "    cj = T.ColorJitter(0.2, 0.2, 0.2, 0.0)\n",
    "    fn_idx, b, c, s, h2 = T.ColorJitter.get_params(cj.brightness, cj.contrast, cj.saturation, cj.hue)\n",
    "    return (i, j, h, w), do_hflip, (fn_idx, b, c, s)\n",
    "\n",
    "def augment_clip(frames):  # frames: [T, C, H, W]\n",
    "    (i, j, h, w), do_hflip, (fn_idx, b, c, s) = _sample_clip_params(frames[0])\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=InterpolationMode.BICUBIC)\n",
    "        if do_hflip: img = TF.hflip(img)\n",
    "        for fn_id in fn_idx:\n",
    "            if fn_id == 0: img = TF.adjust_brightness(img, b)\n",
    "            elif fn_id == 1: img = TF.adjust_contrast(img, c)\n",
    "            elif fn_id == 2: img = TF.adjust_saturation(img, s)\n",
    "            elif fn_id == 3: pass\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def eval_clip(frames):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        img = TF.to_pil_image(f)\n",
    "        img = TF.resize(img, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "        img = TF.center_crop(img, IMG_SIZE)\n",
    "        t = TF.to_tensor(img)\n",
    "        t = TF.normalize(t, [0.45]*3, [0.225]*3)\n",
    "        out.append(t)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N: return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path, num_clips: int, train=True):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    if L == 0:\n",
    "        print(f\"[ê²½ê³ ] í”„ë ˆì„ì´ 0ì¸ ë¹„ë””ì˜¤: {path}\")\n",
    "        # ë¹ˆ í…ì„œ ë°˜í™˜ (í˜¹ì€ raise Exception)\n",
    "        return [torch.zeros((3, NUM_FRAMES, IMG_SIZE, IMG_SIZE), dtype=torch.float32) for _ in range(num_clips)]\n",
    "    seg_edges = np.linspace(0, L, num_clips + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).contiguous()\n",
    "        proc = augment_clip(clip) if train else eval_clip(clip)\n",
    "        clips.append(proc.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "\n",
    "# ======================= Dataset / Loader =======================\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance=\"none\", train=True, verbose=True):\n",
    "        self.train = train\n",
    "        true_samples, false_samples = [], []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            mp4_dir = (root / sub / \"crop_video\")\n",
    "            if mp4_dir.exists():\n",
    "                for p in mp4_dir.glob(\"*.mp4\"):\n",
    "                    (true_samples if label==1 else false_samples).append((p, label))\n",
    "\n",
    "        if balance == \"oversample\":\n",
    "            n_true, n_false = len(true_samples), len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor, remainder = n_true // n_false, n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        if verbose:\n",
    "            print(f\"âœ… {len(self.samples)} samples found in {root} (balance='{balance}')\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        num_clips = TRAIN_CLIPS_PER_VIDEO if self.train else EVAL_CLIPS_PER_VIDEO\n",
    "        clips = load_clip(path, num_clips=num_clips, train=self.train)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "def filesystem_class_weights(train_root: Path, num_classes=2):\n",
    "    true_cnt  = len(list((train_root/\"balanced_true\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    false_cnt = len(list((train_root/\"false\"/\"crop_video\").glob(\"*.mp4\")))\n",
    "    cnts = {1: true_cnt, 0: false_cnt}\n",
    "    total = sum(cnts.values())\n",
    "    if total == 0: return torch.ones(num_classes, dtype=torch.float32)\n",
    "    w = [total / max(1, cnts.get(c, 0)) for c in range(num_classes)]\n",
    "    s = sum(w); w = [wi * (num_classes / s) for wi in w]\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# oversample + class_weight ë™ì‹œ ì‚¬ìš© ê²½ê³  (ê³¼ë²Œì  ìœ„í—˜)\n",
    "if BALANCE_METHOD == \"oversample\":\n",
    "    print(\"âš ï¸  í˜„ì¬ oversample ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. val/testëŠ” í•­ìƒ balance='none'ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "elif BALANCE_METHOD == \"class_weight\":\n",
    "    print(\"â„¹ï¸  í˜„ì¬ class_weight ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. oversampleê³¼ ë™ì‹œ ì‚¬ìš©ì€ í”¼í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 1) ì „ì²´ íŠ¸ë ˆì¸ ì„¸íŠ¸(ì¦ê°•/ì˜¤ë²„ìƒ˜í”Œ ì˜µì…˜ í¬í•¨)\n",
    "train_full = GolfSwingDataset(\n",
    "    TRAIN_ROOT,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\"),\n",
    "    train=True,\n",
    "    verbose=not CLEAN_OUTPUT\n",
    ")\n",
    "\n",
    "# 2) ì¸ë±ìŠ¤ ë¶„í• ë§Œ ì–»ê¸°\n",
    "val_len = max(1, int(len(train_full) * VAL_SPLIT))\n",
    "train_len = len(train_full) - val_len\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(train_full, [train_len, val_len], generator=g)\n",
    "\n",
    "# 3) ë™ì¼í•œ ìƒ˜í”Œ ëª©ë¡ì„ ê³µìœ í•˜ë˜, ì„œë¡œ ë‹¤ë¥¸ 'train í”Œë˜ê·¸'ë¥¼ ê°–ëŠ” ë³„ë„ Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "def clone_dataset_with_indices(src_ds: GolfSwingDataset, indices, train_flag: bool, balance: str):\n",
    "    ds = GolfSwingDataset(TRAIN_ROOT, balance=balance, train=train_flag, verbose=False)\n",
    "    ds.samples = [src_ds.samples[i] for i in indices]   # ì›ë³¸ì—ì„œ ì„ íƒëœ ìƒ˜í”Œë§Œ ë³µì‚¬\n",
    "    return ds\n",
    "\n",
    "train_dataset = clone_dataset_with_indices(\n",
    "    train_full, train_subset.indices, train_flag=True,\n",
    "    balance=(\"oversample\" if BALANCE_METHOD==\"oversample\" else \"none\")\n",
    ")\n",
    "val_dataset   = clone_dataset_with_indices(\n",
    "    train_full, val_subset.indices,   train_flag=False, balance=\"none\"\n",
    ")\n",
    "\n",
    "# 4) í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” ì›ë˜ëŒ€ë¡œ (í‰ê°€ ì „ìš©ì´ë¯€ë¡œ train=False)\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=\"none\", train=False, verbose=not CLEAN_OUTPUT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# ======================= Model / Optim =======================\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = getattr(self.base, \"head\", None)\n",
    "        if self.head is None and hasattr(self.base, \"model\"):\n",
    "            self.head = self.base.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.base\n",
    "        if hasattr(base, \"forward_features\"): feats = base.forward_features(x)\n",
    "        elif hasattr(base, \"model\") and hasattr(base.model, \"forward_features\"): feats = base.model.forward_features(x)\n",
    "        else: feats = base(x)\n",
    "        return self.head(self.dropout(feats))\n",
    "\n",
    "base_model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES, num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PTH)\n",
    ").to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=DROPOUT).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model, train_blocks: int):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    if train_blocks == -1:\n",
    "        for p in base.parameters(): p.requires_grad = True\n",
    "        return [p for p in base.parameters() if p.requires_grad]\n",
    "    # Freeze all\n",
    "    for p in base.parameters(): p.requires_grad = False\n",
    "    # Unfreeze head\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    if head is not None:\n",
    "        for p in head.parameters(): p.requires_grad = True\n",
    "    # Unfreeze last N blocks\n",
    "    blocks = getattr(inner, \"blocks\", None)\n",
    "    if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "        total = len(blocks); start = max(0, total - train_blocks)\n",
    "        for b in blocks[start:]:\n",
    "            for p in b.parameters(): p.requires_grad = True\n",
    "    return [p for p in base.parameters() if p.requires_grad]\n",
    "\n",
    "def build_optimizer(model, train_blocks: int, base_lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "    base = model.base if hasattr(model, \"base\") else model\n",
    "    inner = base.model if hasattr(base, \"model\") else base\n",
    "    head = getattr(inner, \"head\", None)\n",
    "    head_params = list(head.parameters()) if head is not None else []\n",
    "\n",
    "    if train_blocks == -1:\n",
    "        head_set = set(head_params)\n",
    "        backbone_params = [p for p in base.parameters() if p.requires_grad and p not in head_set]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,     \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * 0.5, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "    else:\n",
    "        blocks = getattr(inner, \"blocks\", None)\n",
    "        block_params = []\n",
    "        if blocks is not None and isinstance(blocks, (list, nn.ModuleList, nn.Sequential)):\n",
    "            total = len(blocks); start = max(0, total - train_blocks)\n",
    "            for b in blocks[start:]:\n",
    "                block_params += [p for p in b.parameters() if p.requires_grad]\n",
    "        param_groups = [\n",
    "            {\"params\": head_params,  \"lr\": base_lr * 2.0, \"weight_decay\": weight_decay},\n",
    "            {\"params\": block_params, \"lr\": base_lr * 1.0, \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(param_groups)\n",
    "    # === LR ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ê·¸ë£¹ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë„ë¡ initial_lr ì €ì¥ ===\n",
    "    for g in opt.param_groups:\n",
    "        g[\"initial_lr\"] = g[\"lr\"]\n",
    "    return opt\n",
    "\n",
    "trainable_params = get_trainable_params(model, TRAIN_BLOCK)\n",
    "optimizer = build_optimizer(model, TRAIN_BLOCK, base_lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class weights (ì˜µì…˜)\n",
    "if BALANCE_METHOD == \"class_weight\":\n",
    "    class_w = filesystem_class_weights(TRAIN_ROOT).to(DEVICE)\n",
    "else:\n",
    "    class_w = None\n",
    "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# AMP & LR schedule â€” ê·¸ë£¹ ë¹„ìœ¨ ìœ ì§€í˜• ìŠ¤ì¼€ì¼ íŒ©í„°\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def get_lr_factor(epoch):\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    t = epoch - WARMUP_EPOCHS\n",
    "    T_total = max(1, EPOCHS - WARMUP_EPOCHS)\n",
    "    return 0.5 * (1 + np.cos(np.pi * t / T_total))\n",
    "\n",
    "def set_lr_with_factor(optimr, factor):\n",
    "    for g in optimr.param_groups:\n",
    "        g['lr'] = g['initial_lr'] * factor\n",
    "\n",
    "# ======================= Train / Eval =======================\n",
    "def video_level_from_logits(logits, B, clips_per_video):\n",
    "    return logits.view(B, clips_per_video, -1).mean(dim=1)\n",
    "\n",
    "train_logs = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": []}\n",
    "val_acc_per_epoch, best_val_acc, patience = [], -1.0, 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    lr_factor = get_lr_factor(epoch)\n",
    "    set_lr_with_factor(optimizer, lr_factor)\n",
    "\n",
    "    total_videos = correct_videos = 0\n",
    "    running_loss, running_steps = 0.0, 0\n",
    "\n",
    "    bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\",\n",
    "            ncols=80, leave=False, disable=not SHOW_TRAIN_BAR)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)  # âœ… ë£¨í”„ ì‹œì‘ ì „ì— 1ë²ˆë§Œ\n",
    "\n",
    "    for step, (clips, label) in enumerate(bar):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)               # [B*C, C,T,H,W]\n",
    "        labs = label.repeat_interleave(TRAIN_CLIPS_PER_VIDEO).to(DEVICE) # ë¼ë²¨ í™•ì¥\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)                                            # [B*C, 2]\n",
    "            loss = criterion(outs, labs)\n",
    "\n",
    "        # âœ… ëˆ„ì : ìŠ¤ì¼€ì¼í•œ lossë¥¼ ë‚˜ëˆ ì„œ backward\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "\n",
    "        # ë¡œê¹…ì€ ì›ë˜ loss ê¸°ì¤€ìœ¼ë¡œ\n",
    "        running_loss += loss.item()\n",
    "        running_steps += 1\n",
    "\n",
    "        # ë¹„ë””ì˜¤ ë‹¨ìœ„ ì •í™•ë„ ê°±ì‹ (ë¡œì§“ í‰ê· )\n",
    "        with torch.no_grad():\n",
    "            vid_logits = video_level_from_logits(outs, B, TRAIN_CLIPS_PER_VIDEO)\n",
    "            preds = vid_logits.argmax(1).cpu()\n",
    "            correct_videos += (preds == label).sum().item()\n",
    "            total_videos   += B\n",
    "\n",
    "        # âœ… ëˆ„ì  stepë§ˆë‹¤ë§Œ step/zero_grad\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # AMP ì‚¬ìš© ì‹œ, clip ì „ì— unscale ê¶Œì¥\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if SHOW_TRAIN_BAR:\n",
    "            bar.set_postfix({\n",
    "                \"Loss\": f\"{running_loss/max(1,running_steps):.4f}\",\n",
    "                \"Acc\":  f\"{correct_videos/max(1,total_videos):.2%}\"\n",
    "            })\n",
    "\n",
    "    # âœ… ë§ˆì§€ë§‰ì— ë‚¨ì€ ê·¸ë¼ë””ì–¸íŠ¸ ì²˜ë¦¬(ë¯¸ì™„ ë°°ì¹˜ê°€ ìˆì„ ë•Œ)\n",
    "    remainder = (step + 1) % GRAD_ACCUM_STEPS\n",
    "    if remainder != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    train_acc = correct_videos / max(1, total_videos)\n",
    "    avg_loss  = running_loss / max(1, running_steps)\n",
    "    train_logs[\"train_acc\"].append(train_acc); train_logs[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval(); all_preds, all_labels = [], []\n",
    "    val_total = val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        vbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\",\n",
    "                    ncols=80, leave=False, disable=not SHOW_EVAL_BAR)\n",
    "        for clips, label in vbar:\n",
    "            B = clips.shape[0]\n",
    "            vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "            with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "                outs = model(vids)\n",
    "                vid_logits = video_level_from_logits(outs, B, EVAL_CLIPS_PER_VIDEO)\n",
    "                pred = vid_logits.argmax(1).cpu()\n",
    "            val_total += B; val_correct += (pred == label).sum().item()\n",
    "            if SHOW_EVAL_BAR: vbar.set_postfix({\"Acc\": f\"{val_correct/max(1,val_total):.2%}\"})\n",
    "            all_preds.extend(pred.numpy()); all_labels.extend(label.numpy())\n",
    "\n",
    "    val_acc = val_correct / max(1, val_total)\n",
    "    train_logs[\"val_acc\"].append(val_acc); val_acc_per_epoch.append(val_acc)\n",
    "\n",
    "    if PLOT_CM_EVERY_EPOCH:\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "        disp.plot(cmap=\"Blues\"); plt.title(f\"Val Confusion Matrix (Epoch {epoch+1})\"); plt.show()\n",
    "\n",
    "    # ---- Early stop / Save best (val ê¸°ì¤€)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc, patience = val_acc, 0\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict(),\n",
    "                    \"opt\": optimizer.state_dict(), \"metrics\": {\"val_acc\": val_acc}}, BEST_CKPT)\n",
    "        if not CLEAN_OUTPUT or PRINT_EPOCH_SAVE:\n",
    "            print(f\"  âœ… Best updated: val_acc={val_acc:.3%} -> saved to {BEST_CKPT}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= EARLY_STOP:\n",
    "            print(\"â›³ Early stopping triggered.\"); break\n",
    "\n",
    "    if CLEAN_OUTPUT:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | train_acc={train_acc:.3%} loss={avg_loss:.4f} \"\n",
    "              f\"| val_acc={val_acc:.3%} | best={best_val_acc:.3%}\")\n",
    "\n",
    "    with open(\"train_logs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_logs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 144/257 [04:59<03:45,  1.99s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================= Final Test (ì •í™•ë„ + ë¶ˆê· í˜• ì§€í‘œ) =======================\n",
    "# í•„ìš”ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ (í•™ìŠµ í›„ ë°”ë¡œ í…ŒìŠ¤íŠ¸í•  ê²½ìš° ìƒëµ ê°€ëŠ¥)\n",
    "model.load_state_dict(torch.load(BEST_CKPT)[\"model\"])\n",
    "\n",
    "model.eval(); all_preds, all_labels, all_scores = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(test_loader, desc=\"[Test]\", ncols=80, disable=not SHOW_EVAL_BAR):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)  # [B*C, 2]\n",
    "            # ë™ì ìœ¼ë¡œ clips_per_video ê³„ì‚°\n",
    "            clips_per_video = outs.shape[0] // B\n",
    "            vid_logits = outs.view(B, clips_per_video, -1).mean(dim=1)  # [B,2]\n",
    "            probs = F.softmax(vid_logits, dim=1)  # í™•ë¥  ìŠ¤ì½”ì–´\n",
    "            pred = vid_logits.argmax(1).cpu()\n",
    "\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "        all_scores.extend(probs[:, 1].cpu().numpy())  # ì–‘ì„±( true ) í™•ë¥ , í•„ìš” ì‹œ falseë©´ [:,0]\n",
    "\n",
    "# ì›ë˜ accuracy\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\nâœ… Test Accuracy: {test_acc:.3%}\")\n",
    "\n",
    "# ë¶ˆê· í˜• ì¹œí™” ì§€í‘œ\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "# ROC-AUC: ì–‘ì„± í´ë˜ìŠ¤ë¥¼ 1ë¡œ ê°€ì •\n",
    "try:\n",
    "    roc_auc = roc_auc_score(all_labels, np.array(all_scores))\n",
    "except ValueError:\n",
    "    roc_auc = float('nan')\n",
    "# PR-AUC (ì–‘ì„± ê¸°ì¤€)\n",
    "ap = average_precision_score(all_labels, np.array(all_scores))\n",
    "\n",
    "print(f\"ğŸ” Balanced Acc : {bal_acc:.3%}\")\n",
    "print(f\"ğŸ” Macro F1     : {macro_f1:.3%}\")\n",
    "print(f\"ğŸ” ROC-AUC      : {roc_auc:.3f}\")\n",
    "print(f\"ğŸ” PR-AUC(AP)   : {ap:.3f}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ì •ë°€ë„/ì¬í˜„ìœ¨/F1\n",
    "prec, rec, f1, supp = precision_recall_fscore_support(all_labels, all_preds, labels=[0,1], zero_division=0)\n",
    "print(\"\\n[Per-class]\")\n",
    "print(f\" false(0) -> P:{prec[0]:.3f} R:{rec[0]:.3f} F1:{f1[0]:.3f} (n={supp[0]})\")\n",
    "print(f\" true (1) -> P:{prec[1]:.3f} R:{rec[1]:.3f} F1:{f1[1]:.3f} (n={supp[1]})\")\n",
    "\n",
    "# ì •ê·œí™”ëœ í˜¼ë™í–‰ë ¬(ë¹„ìœ¨)\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1], normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\"); plt.title(\"Test CM (Normalized)\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432714d",
   "metadata": {},
   "source": [
    "# true, false 1ëŒ€1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# ======================= 1:1 (false:true) Test Evaluation =======================\n",
    "model.load_state_dict(torch.load(BEST_CKPT)[\"model\"])\n",
    "\n",
    "# 1. test_datasetì—ì„œ false/true ì¸ë±ìŠ¤ ë¶„ë¦¬\n",
    "false_indices = [i for i, (p, l) in enumerate(test_dataset.samples) if l == 0]\n",
    "true_indices  = [i for i, (p, l) in enumerate(test_dataset.samples) if l == 1]\n",
    "\n",
    "# 2. false ê°œìˆ˜ë§Œí¼ trueì—ì„œ ëœë¤ ì¶”ì¶œ\n",
    "random.seed(SEED)\n",
    "n_false = len(false_indices)\n",
    "sampled_true_indices = random.sample(true_indices, min(n_false, len(true_indices)))\n",
    "\n",
    "# 3. í•©ì³ì„œ 1:1 ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì…”í”Œ\n",
    "balanced_indices = false_indices + sampled_true_indices\n",
    "random.shuffle(balanced_indices)\n",
    "\n",
    "# 4. Subset DataLoader ìƒì„±\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "balanced_subset = Subset(test_dataset, balanced_indices)\n",
    "balanced_loader = DataLoader(balanced_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# 5. í‰ê°€\n",
    "model.eval(); all_preds, all_labels, all_scores = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(balanced_loader, desc=\"[Test 1:1]\", ncols=80, disable=not SHOW_EVAL_BAR):\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE==\"cuda\")):\n",
    "            outs = model(vids)\n",
    "            clips_per_video = outs.shape[0] // B\n",
    "            vid_logits = outs.view(B, clips_per_video, -1).mean(dim=1)\n",
    "            probs = F.softmax(vid_logits, dim=1)\n",
    "            pred = vid_logits.argmax(1).cpu()\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "        all_scores.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "# 6. ì§€í‘œ ì¶œë ¥\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "try:\n",
    "    roc_auc = roc_auc_score(all_labels, np.array(all_scores))\n",
    "except ValueError:\n",
    "    roc_auc = float('nan')\n",
    "ap = average_precision_score(all_labels, np.array(all_scores))\n",
    "\n",
    "print(f\"\\nâœ… [1:1] Test Accuracy: {test_acc:.3%}\")\n",
    "print(f\"ğŸ” Balanced Acc : {bal_acc:.3%}\")\n",
    "print(f\"ğŸ” Macro F1     : {macro_f1:.3%}\")\n",
    "print(f\"ğŸ” ROC-AUC      : {roc_auc:.3f}\")\n",
    "print(f\"ğŸ” PR-AUC(AP)   : {ap:.3f}\")\n",
    "\n",
    "prec, rec, f1, supp = precision_recall_fscore_support(all_labels, all_preds, labels=[0,1], zero_division=0)\n",
    "print(\"\\n[Per-class]\")\n",
    "print(f\" false(0) -> P:{prec[0]:.3f} R:{rec[0]:.3f} F1:{f1[0]:.3f} (n={supp[0]})\")\n",
    "print(f\" true (1) -> P:{prec[1]:.3f} R:{rec[1]:.3f} F1:{f1[1]:.3f} (n={supp[1]})\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1], normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"False\",\"True\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\"); plt.title(\"Test 1:1 CM (Normalized)\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
