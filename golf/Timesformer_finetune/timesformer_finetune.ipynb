{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677fb496",
   "metadata": {},
   "source": [
    "# timesformer 파인튜닝  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef110f6",
   "metadata": {},
   "source": [
    "Timesformer-L 모델(K600 데이터셋):  \n",
    "96frames, 224 사이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797badc",
   "metadata": {},
   "source": [
    "Timesformer-HR 모델:  \n",
    "16frames, 448 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2404 samples found in D:\\golfDataset\\dataset\\train (balanced=True)\n",
      "✅ 295 samples found in D:\\golfDataset\\dataset\\test (balanced=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 51.310%, loss: 0.1951\n",
      "  ✔ Model saved: epochs\\epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 202\u001b[39m\n\u001b[32m    200\u001b[39m         pred = probs.argmax(\u001b[32m1\u001b[39m)\n\u001b[32m    201\u001b[39m         test_total += pred.size(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         test_correct += (\u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m).sum().item()\n\u001b[32m    203\u001b[39m         test_bar.set_postfix({\n\u001b[32m    204\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAcc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_correct/test_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         })\n\u001b[32m    206\u001b[39m test_acc = test_correct / test_total \u001b[38;5;28;01mif\u001b[39;00m test_total > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "from timesformer.models.vit import TimeSformer\n",
    "import pickle\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------- 하이퍼파라미터 및 경로 ----------------------------\n",
    "TRAIN_ROOT = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT  = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_16x16_448_K600.pyth\")\n",
    "# Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_16x16_448_K600.pyth\")  # 16프레임 모델 경로\n",
    "NUM_FRAMES = 16\n",
    "CLIPS_PER_VIDEO = 2\n",
    "IMG_SIZE = 448\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.7\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "EPOCHS_DIR = Path(\"epochs\")\n",
    "EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_PATH = Path(\"checkpoint.pth\")\n",
    "\n",
    "# ----------------- 재현성 ----------------------------\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----------------- 전처리 ----------------------------\n",
    "def preprocess_tensor(img_tensor):\n",
    "    img = F.resize(img_tensor, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "    img = F.center_crop(img, IMG_SIZE)\n",
    "    img = F.normalize(img, [0.45]*3, [0.225]*3)\n",
    "    return img\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N:\n",
    "        return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    seg_edges = np.linspace(0, L, CLIPS_PER_VIDEO + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).float() / 255.0\n",
    "        clip = torch.stack([preprocess_tensor(f) for f in clip])\n",
    "        clips.append(clip.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "# ----------------- 모델 ----------------------------\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = base_model.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.base.model.forward_features(x)\n",
    "        out = self.dropout(feats)\n",
    "        return self.head(out)\n",
    "    \n",
    "# ----------------- 데이터셋 ----------------------------\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance):\n",
    "        self.samples = []\n",
    "        true_samples = []\n",
    "        false_samples = []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            for p in (root/sub/\"crop_video\").glob(\"*.mp4\"):\n",
    "                if label == 1:\n",
    "                    true_samples.append((p, 1))\n",
    "                else:\n",
    "                    false_samples.append((p, 0))\n",
    "        if balance:\n",
    "            # false 샘플을 true 샘플 개수만큼 복제 (불균형 보완)\n",
    "            n_true = len(true_samples)\n",
    "            n_false = len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor = n_true // n_false\n",
    "                remainder = n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        print(f\"✅ {len(self.samples)} samples found in {root} (balanced={balance})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clips = load_clip(path)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "# 데이터셋 생성 시 balance 옵션 추가\n",
    "train_dataset = GolfSwingDataset(TRAIN_ROOT, balance=True)  # <-- train에 balance=True로 불균형 보완\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# ----------------- 모델 ----------------------------\n",
    "base_model = TimeSformer(img_size=IMG_SIZE, num_frames=NUM_FRAMES,\n",
    "                        num_classes=2, attention_type='divided_space_time',\n",
    "                        pretrained_model=str(PRETRAIN_PTH)).to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=0.5).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    trainable = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(x in name for x in ('head', 'cls_head')):\n",
    "            param.requires_grad = True\n",
    "            trainable.append(param)\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    return trainable\n",
    "\n",
    "optimizer = optim.AdamW(get_trainable_params(model), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ----------------- Checkpoint 불러오기 ----------------------------\n",
    "start_epoch = 0\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f\"🔁 Loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"opt\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    print(f\"✅ Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# ----------------- 학습 및 각 epoch별 모델 저장/테스트 ----------------------------\n",
    "train_logs = {\"train_acc\": [], \"test_acc\": [], \"train_loss\": []}\n",
    "test_acc_per_epoch = []\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    total = correct = total_loss = 0\n",
    "    batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Batch]\", ncols=70, leave=False)\n",
    "    for batch_idx, (clips, label) in enumerate(batch_bar):\n",
    "        # clips shape: (B, CLIPS_PER_VIDEO, C, T, H, W)\n",
    "        B = clips.shape[0]\n",
    "        clips = clips.view(-1, *clips.shape[2:]).to(DEVICE)  # (B*CLIPS_PER_VIDEO, C, T, H, W)\n",
    "        labs = label.repeat(CLIPS_PER_VIDEO).to(DEVICE)\n",
    "        outs = model(clips)\n",
    "        loss = criterion(outs, labs)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total += labs.size(0)\n",
    "        correct += (outs.argmax(1) == labs).sum().item()\n",
    "        \n",
    "        # 서브 로그: 현재 배치, 누적 loss, 누적 acc\n",
    "        batch_bar.set_postfix({\n",
    "            \"Batch\": batch_idx + 1,\n",
    "            \"Loss\": f\"{total_loss/(batch_idx+1):.4f}\",\n",
    "            \"Acc\": f\"{correct/total:.2%}\"\n",
    "        })\n",
    "\n",
    "    train_acc = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    train_logs[\"train_acc\"].append(train_acc)\n",
    "    train_logs[\"train_loss\"].append(avg_loss)\n",
    "    print(f\"  train acc: {train_acc:.3%}, loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ✔ 각 epoch별 모델 저장\n",
    "    epoch_model_path = EPOCHS_DIR / f\"epoch_{epoch+1}.pth\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"opt\": optimizer.state_dict(),\n",
    "    }, epoch_model_path)\n",
    "    print(f\"  ✔ Model saved: {epoch_model_path}\")\n",
    "\n",
    "    # ✔ 테스트 정확도 기록 (test 데이터셋에서)\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"Epoch {epoch} [Test]\", ncols=70, leave=False)\n",
    "        for clips, label in test_bar:\n",
    "            B = clips.shape[0]\n",
    "            vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)  # (B*CLIPS_PER_VIDEO, C, T, H, W)\n",
    "            labs = label.repeat(CLIPS_PER_VIDEO).to(DEVICE)     # [B*CLIPS_PER_VIDEO]\n",
    "            probs = model(vids).softmax(1)\n",
    "            pred = probs.argmax(1)\n",
    "            test_total += pred.size(0)\n",
    "            test_correct += (pred.cpu() == labs.cpu()).sum().item()\n",
    "            test_bar.set_postfix({\n",
    "                \"Acc\": f\"{test_correct/test_total:.2%}\"\n",
    "            })\n",
    "    test_acc = test_correct / test_total if test_total > 0 else 0\n",
    "    train_logs[\"test_acc\"].append(test_acc)\n",
    "    test_acc_per_epoch.append(test_acc)\n",
    "    print(f\"  test  acc: {test_acc:.3%}\")\n",
    "\n",
    "    # ✔ 로그 저장\n",
    "    with open(\"train_logs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_logs, f)\n",
    "\n",
    "    # ✔ 체크포인트 저장\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"opt\": optimizer.state_dict(),\n",
    "    }, CHECKPOINT_PATH)\n",
    "\n",
    "# 🔧 최종 학습된 모델 저장\n",
    "final_model_path = Path(\"timesformer_finetuned.pth\")\n",
    "torch.save({\n",
    "    \"epoch\": EPOCHS - 1,\n",
    "    \"model\": model.state_dict(),\n",
    "    \"opt\": optimizer.state_dict(),\n",
    "}, final_model_path)\n",
    "print(f\"\\n✅ Final model saved to {final_model_path}\")\n",
    "\n",
    "# ----------------- 전체 epoch별 test 정확도 시각화 ----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, EPOCHS+1), test_acc_per_epoch, marker='o', label=\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd112eab",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 21) (1675854933.py, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"\\n✅ Test\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 21)\n"
     ]
    }
   ],
   "source": [
    "# 체크포인트에서 모델 불러오기\n",
    "checkpoint = torch.load(\"epochs/epoch_1.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "test_total = test_correct = 0\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=\"Test [epoch_1.pth]\", ncols=70)\n",
    "    for clips, label in test_bar:\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        labs = label.repeat(CLIPS_PER_VIDEO).to(DEVICE)\n",
    "        probs = model(vids).softmax(1)\n",
    "        pred = probs.argmax(1)\n",
    "        test_total += pred.size(0)\n",
    "        test_correct += (pred.cpu() == labs.cpu()).sum().item()\n",
    "        test_bar.set_postfix({\n",
    "            \"Acc\": f\"{test_correct/test_total:.2%}\"\n",
    "        })\n",
    "test_acc = test_correct / test_total if test_total > 0 else 0\n",
    "print(f\"\\n✅ Test accuracy (epoch_1.pth): {test_acc:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
