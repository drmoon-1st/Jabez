{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 436 samples found in D:\\golfDataset\\ìŠ¤í¬ì¸  ì‚¬ëŒ ë™ì‘ ì˜ìƒ(ê³¨í”„)\\Training\\Public\\male\\train\n",
      "ğŸ” Loading checkpoint from checkpoint.pth\n",
      "âœ… Resuming from epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [10:46<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 90.941%\n",
      "ğŸ’¾ Checkpoint saved to checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [10:45<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 89.567%\n",
      "ğŸ’¾ Checkpoint saved to checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [10:45<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 92.265%\n",
      "ğŸ’¾ Checkpoint saved to checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [10:46<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 92.163%\n",
      "ğŸ’¾ Checkpoint saved to checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [10:48<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 92.774%\n",
      "ğŸ’¾ Checkpoint saved to checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [01:09<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test Video Accuracy : 74.419%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")  # timesformer ê²½ë¡œë¥¼ python ëª¨ë“ˆ ê²½ë¡œì— ì¶”ê°€\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "def preprocess_tensor(img_tensor):  # img_tensor: (3, H, W)\n",
    "    img = F.resize(img_tensor, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "    img = F.center_crop(img, IMG_SIZE)\n",
    "    img = F.normalize(img, [0.45]*3, [0.225]*3)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- í•˜ì´í¼íŒŒë¼ë¯¸í„° ----------------------------\n",
    "ROOT          = Path(r\"D:\\golfDataset\\ìŠ¤í¬ì¸  ì‚¬ëŒ ë™ì‘ ì˜ìƒ(ê³¨í”„)\\Training\\Public\\male\\train\")\n",
    "PRETRAIN_PYTH = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_96x4_224_K600.pyth\")\n",
    "NUM_FRAMES    = 32\n",
    "CLIPS_PER_VID = 5\n",
    "IMG_SIZE      = 224\n",
    "BATCH         = 4\n",
    "LR            = 1e-3\n",
    "EPOCHS        = 10\n",
    "SEED          = 42\n",
    "TEST_RATIO    = 0.1     # í…ŒìŠ¤íŠ¸ì…‹ ë¹„ìœ¨\n",
    "\n",
    "# ----------------- ì¬í˜„ì„± ----------------------------\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = \"cuda\"  # ë¬´ì¡°ê±´ gpu ì‚¬ìš© ì•ˆë˜ë©´ ì˜¤ë¥˜ ë‚´ë²„ë¦¬ê¸°\n",
    "\n",
    "# ----------------- ì „ì²˜ë¦¬ ----------------------------\n",
    "base_tf = T.Compose([\n",
    "    T.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.45]*3, [0.225]*3),\n",
    "])\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N:\n",
    "        return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path):  # ë¹„ë””ì˜¤ë¥¼ timesformer ì…ë ¥ì— ë§ê²Œ ì „ì²˜ë¦¬\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    seg_edges = np.linspace(0, L, CLIPS_PER_VID + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)              # (T, H, W, 3)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).float() / 255.0  # (T, 3, H, W)\n",
    "        clip = torch.stack([preprocess_tensor(f) for f in clip])         # (T, 3, H, W)\n",
    "        clips.append(clip.permute(1, 0, 2, 3))                            # (3, T, H, W)\n",
    "    return clips  # list of (3, T, H, W)\n",
    "\n",
    "\n",
    "# ----------------- ë°ì´í„°ì…‹ ----------------------------\n",
    "class SwingDataset(Dataset):\n",
    "    def __init__(self, root: Path):\n",
    "        self.samples = []\n",
    "        for lbl, sub in enumerate((\"balanced_true\", \"false\")):\n",
    "            for p in (root/sub/\"crop_video\").glob(\"*.mp4\"):\n",
    "                self.samples.append((p, lbl))\n",
    "        print(f\"\\u2705 {len(self.samples)} samples found in {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, lbl = self.samples[i]\n",
    "        clips = load_clip(path)\n",
    "        return torch.stack(clips), torch.tensor(lbl)\n",
    "\n",
    "# ----------------- ë°ì´í„° ë¡œë” ----------------------------\n",
    "ds_full = SwingDataset(ROOT)\n",
    "n_test = int(len(ds_full)*TEST_RATIO)\n",
    "n_train = len(ds_full) - n_test\n",
    "train_ds, test_ds = random_split(ds_full, [n_train, n_test])\n",
    "train_ld = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "# num_workersëŠ” CPU ì½”ì–´ìˆ˜, 4ê°œë¡œ í•˜ë‹ˆ ì˜¤ë¥˜ìƒê²¨ì„œ ì¼ë‹¨ 0ê°œë¡œ ì„¤ì •\n",
    "\n",
    "# ----------------- Checkpoint ì„¤ì • ----------------------------\n",
    "checkpoint_path = Path(\"checkpoint.pth\")  # ğŸ”§ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "start_epoch = 0  # ğŸ”§ ê¸°ë³¸ê°’\n",
    "\n",
    "# ----------------- ëª¨ë¸ ----------------------------\n",
    "model = TimeSformer(img_size=IMG_SIZE, num_frames=NUM_FRAMES,\n",
    "                    num_classes=2, attention_type='divided_space_time',\n",
    "                    pretrained_model=str(PRETRAIN_PYTH)).to(device)\n",
    "\n",
    "# ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •\n",
    "def get_trainable_params(model):\n",
    "    trainable = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(x in name for x in ('head', 'cls_head')):\n",
    "            param.requires_grad = True\n",
    "            trainable.append(param)\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    return trainable\n",
    "\n",
    "opt = optim.AdamW(get_trainable_params(model), lr=LR, weight_decay=0.02)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# ğŸ”§ Checkpointê°€ ì¡´ì¬í•˜ë©´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"ğŸ” Loading checkpoint from {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    opt.load_state_dict(ckpt[\"opt\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    print(f\"âœ… Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# ----------------- í•™ìŠµ ----------------------------\n",
    "for ep in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    tot = correct = 0\n",
    "    for clips, lab in tqdm(train_ld, desc=f\"Epoch {ep}\", ncols=70):\n",
    "        vids = clips.squeeze(0).to(device)  # (5,3,T,H,W)\n",
    "        labs = lab.repeat(CLIPS_PER_VID).to(device)\n",
    "        outs = model(vids)  # (5,2)\n",
    "        loss = crit(outs, labs)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += labs.size(0); correct += (outs.argmax(1) == labs).sum().item()\n",
    "    print(f\"  train acc: {correct/tot:.3%}\")\n",
    "\n",
    "    # ğŸ”§ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    torch.save({\n",
    "        \"epoch\": ep,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"opt\": opt.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"ğŸ’¾ Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# ----------------- í‰ê°€ ----------------------------\n",
    "model.eval(); tot = correct = 0\n",
    "with torch.no_grad():\n",
    "    for clips, lab in tqdm(test_ld, desc=\"[Test]\", ncols=70):\n",
    "        vids = clips.squeeze(0).to(device)\n",
    "        probs = model(vids).softmax(1).mean(0, keepdim=True)  # í‰ê·  ensemble\n",
    "        pred = probs.argmax(1)\n",
    "        tot += 1; correct += (pred.cpu() == lab).item()\n",
    "print(f\"\\nâœ… Test Video Accuracy : {correct/tot:.3%}\")\n",
    "\n",
    "# ğŸ”§ ìµœì¢… í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "final_model_path = Path(\"timesformer_finetuned.pth\")\n",
    "torch.save({\n",
    "    \"epoch\": EPOCHS - 1,\n",
    "    \"model\": model.state_dict(),\n",
    "    \"opt\": opt.state_dict(),\n",
    "}, final_model_path)\n",
    "print(f\"\\nâœ… Final model saved to {final_model_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
