{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677fb496",
   "metadata": {},
   "source": [
    "# timesformer ÌååÏù∏ÌäúÎãù  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef110f6",
   "metadata": {},
   "source": [
    "Timesformer-L Î™®Îç∏(K600 Îç∞Ïù¥ÌÑ∞ÏÖã):  \n",
    "96frames, 224 ÏÇ¨Ïù¥Ï¶à"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797badc",
   "metadata": {},
   "source": [
    "Timesformer-HR Î™®Îç∏:  \n",
    "16frames, 448 ÏÇ¨Ïù¥Ï¶à"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2404 samples found in D:\\golfDataset\\dataset\\train (balanced=True)\n",
      "‚úÖ 295 samples found in D:\\golfDataset\\dataset\\test (balanced=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train acc: 51.310%, loss: 0.1951\n",
      "  ‚úî Model saved: epochs\\epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 202\u001b[39m\n\u001b[32m    200\u001b[39m         pred = probs.argmax(\u001b[32m1\u001b[39m)\n\u001b[32m    201\u001b[39m         test_total += pred.size(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         test_correct += (\u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m).sum().item()\n\u001b[32m    203\u001b[39m         test_bar.set_postfix({\n\u001b[32m    204\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAcc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_correct/test_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         })\n\u001b[32m    206\u001b[39m test_acc = test_correct / test_total \u001b[38;5;28;01mif\u001b[39;00m test_total > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "from timesformer.models.vit import TimeSformer\n",
    "import pickle\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------- ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Î∞è Í≤ΩÎ°ú ----------------------------\n",
    "TRAIN_ROOT = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "TEST_ROOT  = Path(r\"D:\\golfDataset\\dataset\\test\")\n",
    "PRETRAIN_PTH = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_16x16_448_K600.pyth\")\n",
    "# Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_16x16_448_K600.pyth\")  # 16ÌîÑÎ†àÏûÑ Î™®Îç∏ Í≤ΩÎ°ú\n",
    "NUM_FRAMES = 16\n",
    "CLIPS_PER_VIDEO = 2\n",
    "IMG_SIZE = 448\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.7\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "EPOCHS_DIR = Path(\"epochs\")\n",
    "EPOCHS_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_PATH = Path(\"checkpoint.pth\")\n",
    "\n",
    "# ----------------- Ïû¨ÌòÑÏÑ± ----------------------------\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----------------- Ï†ÑÏ≤òÎ¶¨ ----------------------------\n",
    "def preprocess_tensor(img_tensor):\n",
    "    img = F.resize(img_tensor, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "    img = F.center_crop(img, IMG_SIZE)\n",
    "    img = F.normalize(img, [0.45]*3, [0.225]*3)\n",
    "    return img\n",
    "\n",
    "def uniform_sample(L, N):\n",
    "    if L >= N:\n",
    "        return np.linspace(0, L-1, N).astype(int)\n",
    "    return np.pad(np.arange(L), (0, N-L), mode='edge')\n",
    "\n",
    "def load_clip(path: Path):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    seg_edges = np.linspace(0, L, CLIPS_PER_VIDEO + 1, dtype=int)\n",
    "    clips = []\n",
    "    for s0, s1 in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(s1 - s0, NUM_FRAMES) + s0\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)\n",
    "        clip = torch.from_numpy(arr).permute(0, 3, 1, 2).float() / 255.0\n",
    "        clip = torch.stack([preprocess_tensor(f) for f in clip])\n",
    "        clips.append(clip.permute(1, 0, 2, 3))\n",
    "    return clips\n",
    "# ----------------- Î™®Îç∏ ----------------------------\n",
    "class TimeSformerWithDropout(nn.Module):\n",
    "    def __init__(self, base_model, dropout_p=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.head = base_model.model.head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.base.model.forward_features(x)\n",
    "        out = self.dropout(feats)\n",
    "        return self.head(out)\n",
    "    \n",
    "# ----------------- Îç∞Ïù¥ÌÑ∞ÏÖã ----------------------------\n",
    "class GolfSwingDataset(Dataset):\n",
    "    def __init__(self, root: Path, balance):\n",
    "        self.samples = []\n",
    "        true_samples = []\n",
    "        false_samples = []\n",
    "        for label, sub in [(1, \"balanced_true\"), (0, \"false\")]:\n",
    "            for p in (root/sub/\"crop_video\").glob(\"*.mp4\"):\n",
    "                if label == 1:\n",
    "                    true_samples.append((p, 1))\n",
    "                else:\n",
    "                    false_samples.append((p, 0))\n",
    "        if balance:\n",
    "            # false ÏÉòÌîåÏùÑ true ÏÉòÌîå Í∞úÏàòÎßåÌÅº Î≥µÏ†ú (Î∂àÍ∑†Ìòï Î≥¥ÏôÑ)\n",
    "            n_true = len(true_samples)\n",
    "            n_false = len(false_samples)\n",
    "            if n_false > 0 and n_true > n_false:\n",
    "                factor = n_true // n_false\n",
    "                remainder = n_true % n_false\n",
    "                false_samples = false_samples * factor + false_samples[:remainder]\n",
    "        self.samples = true_samples + false_samples\n",
    "        random.shuffle(self.samples)\n",
    "        print(f\"‚úÖ {len(self.samples)} samples found in {root} (balanced={balance})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clips = load_clip(path)\n",
    "        return torch.stack(clips), torch.tensor(label)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± Ïãú balance ÏòµÏÖò Ï∂îÍ∞Ä\n",
    "train_dataset = GolfSwingDataset(TRAIN_ROOT, balance=True)  # <-- trainÏóê balance=TrueÎ°ú Î∂àÍ∑†Ìòï Î≥¥ÏôÑ\n",
    "test_dataset  = GolfSwingDataset(TEST_ROOT, balance=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# ----------------- Î™®Îç∏ ----------------------------\n",
    "base_model = TimeSformer(img_size=IMG_SIZE, num_frames=NUM_FRAMES,\n",
    "                        num_classes=2, attention_type='divided_space_time',\n",
    "                        pretrained_model=str(PRETRAIN_PTH)).to(DEVICE)\n",
    "model = TimeSformerWithDropout(base_model, dropout_p=0.5).to(DEVICE)\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    trainable = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(x in name for x in ('head', 'cls_head')):\n",
    "            param.requires_grad = True\n",
    "            trainable.append(param)\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    return trainable\n",
    "\n",
    "optimizer = optim.AdamW(get_trainable_params(model), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ----------------- Checkpoint Î∂àÎü¨Ïò§Í∏∞ ----------------------------\n",
    "start_epoch = 0\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f\"üîÅ Loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"opt\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    print(f\"‚úÖ Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# ----------------- ÌïôÏäµ Î∞è Í∞Å epochÎ≥Ñ Î™®Îç∏ Ï†ÄÏû•/ÌÖåÏä§Ìä∏ ----------------------------\n",
    "train_logs = {\"train_acc\": [], \"test_acc\": [], \"train_loss\": []}\n",
    "test_acc_per_epoch = []\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    total = correct = total_loss = 0\n",
    "    batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Batch]\", ncols=70, leave=False)\n",
    "    for batch_idx, (clips, label) in enumerate(batch_bar):\n",
    "        # clips shape: (B, CLIPS_PER_VIDEO, C, T, H, W)\n",
    "        B = clips.shape[0]\n",
    "        clips = clips.view(-1, *clips.shape[2:]).to(DEVICE)  # (B*CLIPS_PER_VIDEO, C, T, H, W)\n",
    "        labs = label.repeat(CLIPS_PER_VIDEO).to(DEVICE)\n",
    "        outs = model(clips)\n",
    "        loss = criterion(outs, labs)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total += labs.size(0)\n",
    "        correct += (outs.argmax(1) == labs).sum().item()\n",
    "        \n",
    "        # ÏÑúÎ∏å Î°úÍ∑∏: ÌòÑÏû¨ Î∞∞Ïπò, ÎàÑÏ†Å loss, ÎàÑÏ†Å acc\n",
    "        batch_bar.set_postfix({\n",
    "            \"Batch\": batch_idx + 1,\n",
    "            \"Loss\": f\"{total_loss/(batch_idx+1):.4f}\",\n",
    "            \"Acc\": f\"{correct/total:.2%}\"\n",
    "        })\n",
    "\n",
    "    train_acc = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    train_logs[\"train_acc\"].append(train_acc)\n",
    "    train_logs[\"train_loss\"].append(avg_loss)\n",
    "    print(f\"  train acc: {train_acc:.3%}, loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ‚úî Í∞Å epochÎ≥Ñ Î™®Îç∏ Ï†ÄÏû•\n",
    "    epoch_model_path = EPOCHS_DIR / f\"epoch_{epoch+1}.pth\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"opt\": optimizer.state_dict(),\n",
    "    }, epoch_model_path)\n",
    "    print(f\"  ‚úî Model saved: {epoch_model_path}\")\n",
    "\n",
    "    # ‚úî ÌÖåÏä§Ìä∏ Ï†ïÌôïÎèÑ Í∏∞Î°ù (test Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú)\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"Epoch {epoch} [Test]\", ncols=70, leave=False)\n",
    "        for clips, label in test_bar:\n",
    "            B = clips.shape[0]\n",
    "            vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)  # (B*CLIPS_PER_VIDEO, C, T, H, W)\n",
    "            labs = label.repeat(CLIPS_PER_VIDEO).to(DEVICE)     # [B*CLIPS_PER_VIDEO]\n",
    "            probs = model(vids).softmax(1)\n",
    "            pred = probs.argmax(1)\n",
    "            test_total += pred.size(0)\n",
    "            test_correct += (pred.cpu() == labs.cpu()).sum().item()\n",
    "            test_bar.set_postfix({\n",
    "                \"Acc\": f\"{test_correct/test_total:.2%}\"\n",
    "            })\n",
    "    test_acc = test_correct / test_total if test_total > 0 else 0\n",
    "    train_logs[\"test_acc\"].append(test_acc)\n",
    "    test_acc_per_epoch.append(test_acc)\n",
    "    print(f\"  test  acc: {test_acc:.3%}\")\n",
    "\n",
    "    # ‚úî Î°úÍ∑∏ Ï†ÄÏû•\n",
    "    with open(\"train_logs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_logs, f)\n",
    "\n",
    "    # ‚úî Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"opt\": optimizer.state_dict(),\n",
    "    }, CHECKPOINT_PATH)\n",
    "\n",
    "# üîß ÏµúÏ¢Ö ÌïôÏäµÎêú Î™®Îç∏ Ï†ÄÏû•\n",
    "final_model_path = Path(\"timesformer_finetuned.pth\")\n",
    "torch.save({\n",
    "    \"epoch\": EPOCHS - 1,\n",
    "    \"model\": model.state_dict(),\n",
    "    \"opt\": optimizer.state_dict(),\n",
    "}, final_model_path)\n",
    "print(f\"\\n‚úÖ Final model saved to {final_model_path}\")\n",
    "\n",
    "# ----------------- Ï†ÑÏ≤¥ epochÎ≥Ñ test Ï†ïÌôïÎèÑ ÏãúÍ∞ÅÌôî ----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, EPOCHS+1), test_acc_per_epoch, marker='o', label=\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd112eab",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 21) (1675854933.py, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"\\n‚úÖ Test\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 21)\n"
     ]
    }
   ],
   "source": [
    "# Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "checkpoint = torch.load(\"epochs/epoch_1.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "test_total = test_correct = 0\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc=\"Test [epoch_1.pth]\", ncols=70)\n",
    "    for clips, label in test_bar:\n",
    "        B = clips.shape[0]\n",
    "        vids = clips.view(-1, *clips.shape[2:]).to(DEVICE)\n",
    "        labs = label.repeat(CLIPS_PER_VIDEO).to(DEVICE)\n",
    "        probs = model(vids).softmax(1)\n",
    "        pred = probs.argmax(1)\n",
    "        test_total += pred.size(0)\n",
    "        test_correct += (pred.cpu() == labs.cpu()).sum().item()\n",
    "        test_bar.set_postfix({\n",
    "            \"Acc\": f\"{test_correct/test_total:.2%}\"\n",
    "        })\n",
    "test_acc = test_correct / test_total if test_total > 0 else 0\n",
    "print(f\"\\n‚úÖ Test accuracy (epoch_1.pth): {test_acc:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
