{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrote train.pkl (1621 samples)\n",
      "[INFO] Wrote test.pkl (289 samples)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "# 작업 디렉토리를 crop_pkl로 변경\n",
    "ROOT = r\"D:\\golfDataset\\dataset\\crop_pkl\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "def load_all_annotations(pkl_path):\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data['annotations']\n",
    "\n",
    "def make_mmaction_pkl(train_pkl_list, valid_pkl_list, test_pkl_list, out_train, out_test):\n",
    "    # 1. train/valid\n",
    "    train_annos, train_dirs, valid_dirs = [], [], []\n",
    "    for pkl in train_pkl_list:\n",
    "        annos = load_all_annotations(pkl)\n",
    "        train_annos.extend(annos)\n",
    "        train_dirs.extend([anno['frame_dir'] for anno in annos])\n",
    "    for pkl in valid_pkl_list:\n",
    "        annos = load_all_annotations(pkl)\n",
    "        train_annos.extend(annos)\n",
    "        valid_dirs.extend([anno['frame_dir'] for anno in annos])\n",
    "    split = {\n",
    "        'xsub_train': train_dirs,\n",
    "        'xsub_val': valid_dirs\n",
    "    }\n",
    "    train_data = {\n",
    "        'split': split,\n",
    "        'annotations': train_annos\n",
    "    }\n",
    "    with open(out_train, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    print(f\"[INFO] Wrote train.pkl ({len(train_annos)} samples)\")\n",
    "\n",
    "    # 2. test\n",
    "    test_annos, test_dirs = [], []\n",
    "    for pkl in test_pkl_list:\n",
    "        annos = load_all_annotations(pkl)\n",
    "        test_annos.extend(annos)\n",
    "        test_dirs.extend([anno['frame_dir'] for anno in annos])\n",
    "    split = {\n",
    "        'xsub_val': test_dirs\n",
    "    }\n",
    "    test_data = {\n",
    "        'split': split,\n",
    "        'annotations': test_annos\n",
    "    }\n",
    "    with open(out_test, 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    print(f\"[INFO] Wrote test.pkl ({len(test_annos)} samples)\")\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == '__main__':\n",
    "    # 실제 파일명에 맞게 경로를 지정하세요\n",
    "    train_pkl_list = ['skeleton_dataset_train.pkl']\n",
    "    valid_pkl_list = ['skeleton_dataset_valid.pkl']\n",
    "    test_pkl_list  = ['skeleton_dataset_test.pkl']\n",
    "\n",
    "    make_mmaction_pkl(\n",
    "        train_pkl_list, valid_pkl_list, test_pkl_list,\n",
    "        out_train='train.pkl',\n",
    "        out_test='test.pkl'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c9a4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved: D:\\golfDataset\\dataset\\crop_pkl\\train_unnorm.pkl\n",
      "[INFO] Saved: D:\\golfDataset\\dataset\\crop_pkl\\test_unnorm.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 파일 경로 지정\n",
    "PKL_PATHS = [\n",
    "    (\"D:\\\\golfDataset\\\\dataset\\\\crop_pkl\\\\train.pkl\", \"D:\\\\golfDataset\\\\dataset\\\\crop_pkl\\\\train_unnorm.pkl\"),\n",
    "    (\"D:\\\\golfDataset\\\\dataset\\\\crop_pkl\\\\test.pkl\",  \"D:\\\\golfDataset\\\\dataset\\\\crop_pkl\\\\test_unnorm.pkl\"),\n",
    "]\n",
    "\n",
    "def unnormalize_keypoints(annotations, target_mean, target_std):\n",
    "    # annotations: list of dicts, 각 dict에 'keypoint' (T, V, 2)\n",
    "    for ann in annotations:\n",
    "        kp = np.array(ann['keypoint'])  # (T, V, 2)\n",
    "        cur_mean = np.mean(kp)\n",
    "        cur_std = np.std(kp)\n",
    "        kp = (kp - cur_mean) / (cur_std + 1e-8)\n",
    "        kp = kp * target_std + target_mean\n",
    "        ann['keypoint'] = kp\n",
    "    return annotations\n",
    "\n",
    "# ntu60_2d.pkl의 분포에 맞추기\n",
    "target_mean = np.array([367.29764, 188.13676])\n",
    "target_std = np.array([441.61224, 314.72678])\n",
    "\n",
    "for pkl_in, pkl_out in PKL_PATHS:\n",
    "    with open(pkl_in, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    data['annotations'] = unnormalize_keypoints(data['annotations'], target_mean, target_std)\n",
    "    with open(pkl_out, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"[INFO] Saved: {pkl_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
