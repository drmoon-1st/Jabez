#!/usr/bin/env python
"""
finetune_stgcn.py

ST-GCN fine-tuning using an existing combined annotation PKL
and MMAction2 tools/train.py.
Adds --shuffle option to re-split the PKL data into train/val sets
based on a fixed ratio. The shuffled PKL is temporary and deleted after training.
"""

import argparse
import os
import sys
import pickle
import subprocess
import random
from collections import Counter
# import uuid # ì£¼ì„ ì²˜ë¦¬ëœ uuid ëª¨ë“ˆì€ ì‹¤ì œ ì½”ë“œì—ì„œëŠ” ì œê±°í•˜ê±°ë‚˜ ì£¼ì„ í•´ì œí•´ì•¼ í•¨

# --- Global Configuration ---
# MMAction2 ë£¨íŠ¸ ê²½ë¡œ (tools/train.py ì ‘ê·¼ìš©)
MM_ROOT = r"D:\mmaction2"
sys.path.append(MM_ROOT)

# Frequently-used defaults (hardcoded for this task)
DEVICE = "cuda:0"
# BATCH_SIZEì™€ LRì€ Config íŒŒì¼ì—ì„œ ì œì–´í•˜ë¯€ë¡œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì œê±°í•¨
VAL_SPLIT = "xsub_val" # MMAction2ì˜ split nameì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
# --shuffle ì˜µì…˜ ì‚¬ìš© ì‹œ, Validation ë°ì´í„°ë¡œ ì‚¬ìš©í•  ë¹„ìœ¨ (10%)
RESPLIT_RATIO = 0.1 

# Fixed paths for this project/task (override here instead of CLI)
DEFAULT_INPUT_PKL = r"D:\golfDataset\crop_pkl\combined_5class.pkl"
DEFAULT_CFG = r"configs\skeleton\stgcnpp\my_stgcnpp.py"
DEFAULT_PRETRAINED = r"D:\mmaction2\checkpoints\stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d_20221228-cd11a691.pth"
DEFAULT_WORK_DIR = r"D:\work_dirs\finetune_stgcn_shuffle"

# ----------------------------------------------------------------------
# PKL Manipulation Helper Functions
# ----------------------------------------------------------------------

def _get_all_annotation_indices(data: dict) -> list:
    """PKL íŒŒì¼ì—ì„œ ëª¨ë“  annotationì˜ ì¸ë±ìŠ¤(0ë¶€í„° N-1)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    anns = data.get('annotations', [])
    return list(range(len(anns)))

def _update_splits(data: dict, train_indices: list, val_indices: list):
    """PKL ë°ì´í„°ì˜ split ì •ë³´ë¥¼ ìƒˆë¡œìš´ train/val ì¸ë±ìŠ¤ë¡œ ë®ì–´ì”ë‹ˆë‹¤."""
    data['split'] = {
        'xsub_train': train_indices,
        'xsub_val': val_indices
        # Test splitì€ ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    }

def resplit_pkl(input_pkl_path: str, ratio: float, seed: int = 42) -> str:
    """
    ê¸°ì¡´ PKL íŒŒì¼ì˜ ëª¨ë“  annotationì„ ratioì— ë”°ë¼ train/valë¡œ ì¬ë¶„í• í•˜ê³ ,
    ì„ì‹œ PKL íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ê·¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"\n[SHUFFLE] Re-splitting data with validation ratio {ratio}...")
    
    with open(input_pkl_path, 'rb') as f:
        data = pickle.load(f)

    # ëª¨ë“  annotation ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    all_indices = _get_all_annotation_indices(data)
    
    # ì„ê¸° (ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ì‚¬ìš©)
    random.seed(seed)
    random.shuffle(all_indices)
    
    # ë¹„ìœ¨ì— ë”°ë¼ ì¸ë±ìŠ¤ ë¶„í• 
    num_total = len(all_indices)
    num_val = int(num_total * ratio)
    
    val_indices = all_indices[:num_val]
    train_indices = all_indices[num_val:]
    
    _update_splits(data, train_indices, val_indices)

    print(f"[SHUFFLE] Total annotations: {num_total}")
    print(f"[SHUFFLE] New Train size: {len(train_indices)} ({(len(train_indices)/num_total)*100:.1f}%)")
    print(f"[SHUFFLE] New Val size: {len(val_indices)} ({(len(val_indices)/num_total)*100:.1f}%)")
    
    # ì„ì‹œ PKL íŒŒì¼ ì €ì¥
    tmp_pkl_dir = 'tmp_pkl'
    os.makedirs(tmp_pkl_dir, exist_ok=True)
    # í˜„ì¬ ì‹œê°„ ë˜ëŠ” UUIDë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ í•œ ì„ì‹œ íŒŒì¼ ì´ë¦„ ìƒì„±
    import uuid
    temp_pkl_path = os.path.join(tmp_pkl_dir, f"{os.path.basename(input_pkl_path).replace('.pkl', '')}_{uuid.uuid4().hex[:8]}_shuffled.pkl")
    
    with open(temp_pkl_path, 'wb') as f:
        pickle.dump(data, f)
        
    print(f"[SHUFFLE] Temporary shuffled PKL saved to: {temp_pkl_path}")
    return temp_pkl_path

# ----------------------------------------------------------------------
# Main Logic
# ----------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser()
    # ğŸš¨ epochs ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê³  Config íŒŒì¼ì˜ MAX_EPOCHSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # parser.add_argument('--epochs', type=int, default=30, help='max_epochs override')
    parser.add_argument('--test-pkl', required=False, default='',
                        help='(optional) test annotation PKL ê²½ë¡œ (omit to skip test override)')
    parser.add_argument('--shuffle', action='store_true',
                        help=f'PKLì˜ train/val splitì„ ì¬ë¶„í• í•˜ê³  ì„ìŠµë‹ˆë‹¤ (ratio={RESPLIT_RATIO}).')
    args = parser.parse_args()

    # Use module-level defaults for frequently-changed values
    # args.epochs = args.epochs # ğŸš¨ ì œê±°
    args.input_pkl = DEFAULT_INPUT_PKL
    args.cfg = DEFAULT_CFG
    args.pretrained = DEFAULT_PRETRAINED
    args.work_dir = DEFAULT_WORK_DIR
    args.device = DEVICE
    # args.batch_size = BATCH_SIZE # ğŸš¨ ì œê±°
    # args.lr = LR                 # ğŸš¨ ì œê±°
    args.val_split = VAL_SPLIT
    
    # ----------------------------------------------------------------------
    # 1. PKL íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬: ì…”í”Œ ì—¬ë¶€ì— ë”°ë¼ ìµœì¢… ì‚¬ìš©í•  PKL ê²½ë¡œ ê²°ì • ë° ì„ì‹œ íŒŒì¼ ì¶”ì 
    # ----------------------------------------------------------------------
    
    final_pkl_path = args.input_pkl
    temp_pkl_path_to_delete = None # ì‚­ì œí•  ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥
    
    # ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ PKL íŒŒì¼(í†µí•© annotation)ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.isfile(args.input_pkl):
        raise FileNotFoundError(f"Annotation PKL not found: {args.input_pkl}")
    
    if args.shuffle:
        final_pkl_path = resplit_pkl(args.input_pkl, RESPLIT_RATIO)
        temp_pkl_path_to_delete = final_pkl_path # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥

    print(f"[OK] Using annotation PKL: {final_pkl_path}")
    
    # ----------------------------------------------------------------------
    # 2. í´ë˜ìŠ¤ ìˆ˜ ì¶”ë¡  ë° ì •ë³´ ì¶œë ¥
    # ----------------------------------------------------------------------
    
    # infer number of classes from PKL (if labels exist) and show distribution
    # (í´ë˜ìŠ¤ ì¶”ë¡ ì€ ì›ë³¸ PKL íŒŒì¼ë¡œ ìˆ˜í–‰í•´ë„ ë¬´ë°©í•¨)
    with open(args.input_pkl, 'rb') as f: 
        data = pickle.load(f)
    anns = data.get('annotations', [])
    labels = [a.get('label') for a in anns if 'label' in a]
    
    n_classes = None
    if labels:
        cnt = Counter(labels)
        # í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ 0ë¶€í„° ì‹œì‘í•œë‹¤ê³  ê°€ì •í•˜ê³  ìµœëŒ€ê°’ + 1ì„ ì‚¬ìš©
        n_classes = max(cnt.keys()) + 1 if cnt else 0 
        print(f"[OK] Detected labels: {dict(cnt)}, n_classes={n_classes}")
    else:
        print("[WARN] No 'label' field found in annotations; will not override num_classes.")

    # ----------------------------------------------------------------------
    # 3. Config íŒŒì¼ ì²˜ë¦¬ ë° ì„ì‹œ ì„¤ì • ìƒì„±
    # ----------------------------------------------------------------------
    
    cfg_path = os.path.join(MM_ROOT, args.cfg.strip())
    if not os.path.isfile(cfg_path):
        raise FileNotFoundError(f"Config not found: {cfg_path}")

    # ì„ì‹œ finetune config ìƒì„± (ì ˆëŒ€ê²½ë¡œ base ìƒì†)
    tmp_cfg_dir = 'tmp_cfg'
    os.makedirs(tmp_cfg_dir, exist_ok=True)
    finetune_cfg = os.path.join(tmp_cfg_dir, 'finetune_stgcn_cfg.py')
    # Windows ê²½ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìŠ¬ë˜ì‹œ(/)ë¡œ í†µì¼
    base_cfg_abs = cfg_path.replace('\\', '/')
    # ğŸš¨ .env, .env.local ë“±ì˜ ë³€ìˆ˜ë¥¼ ë”°ë¡œ ë¶„ë¦¬í•˜ë¼ëŠ” ì§€ì¹¨ì„ ì ìš©í•˜ì—¬,
    # ğŸš¨ ì ˆëŒ€ ê²½ë¡œë¥¼ configì— í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³  base ìƒì†ë§Œ ì‚¬ìš©í•˜ë©°,
    # ğŸš¨ í•„ìš”í•œ ê°’ì€ ì‹¤í–‰ ì‹œ --cfg-optionsë¡œ ì „ë‹¬í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    with open(finetune_cfg, 'w', encoding='utf-8') as f:
        f.write(f"_base_ = ['{base_cfg_abs}']\n")
    print(f"[OK] generated finetune config: {finetune_cfg}")

    # ----------------------------------------------------------------------
    # 4. MMAction2 tools/train.py ì‹¤í–‰ ë° ìë™ ì‚­ì œ
    # ----------------------------------------------------------------------
    
    env = os.environ.copy()
    if args.device.startswith('cuda'):
        env['CUDA_VISIBLE_DEVICES'] = args.device.split(':', 1)[1]

    cmd = [
        sys.executable,
        os.path.join(MM_ROOT, 'tools', 'train.py'),
        finetune_cfg,
        '--work-dir', args.work_dir,
        '--cfg-options',
        # Avoid setting global load_from. Instead set the backbone init_cfg checkpoint.
        f"model.backbone.init_cfg.checkpoint={args.pretrained}",
        
        # â­ï¸ íŠ¸ë ˆì´ë‹/ê²€ì¦ ë°ì´í„°ë¡œ ìµœì¢… ê²°ì •ëœ PKL ê²½ë¡œ ì‚¬ìš©
        f"train_dataloader.dataset.dataset.ann_file={final_pkl_path}",
        f"val_dataloader.dataset.ann_file={final_pkl_path}",
        
        # ê¸°íƒ€ í•™ìŠµ ì„¤ì •
        f"train_dataloader.dataset.dataset.split=xsub_train",
        f"val_dataloader.dataset.split={args.val_split}",
        
        # ğŸš¨ BATCH_SIZE, max_epochs, lr ì˜¤ë²„ë¼ì´ë“œë¥¼ ì œê±°í•˜ê³  Config íŒŒì¼ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ]

    # Test PKL ê²½ë¡œê°€ ì£¼ì–´ì§„ ê²½ìš° ì²˜ë¦¬
    if args.test_pkl:
        if not os.path.isfile(args.test_pkl):
            raise FileNotFoundError(f"Test PKL not found: {args.test_pkl}")
        cmd.extend([f"test_dataloader.dataset.ann_file={args.test_pkl}", f"test_dataloader.dataset.split={args.val_split}"])

    # í´ë˜ìŠ¤ ìˆ˜ê°€ ì¶”ë¡ ëœ ê²½ìš°, ëª¨ë¸ í—¤ë“œ ì—…ë°ì´íŠ¸
    if n_classes is not None:
        cmd.extend([f"model.cls_head.num_classes={n_classes}"])

    print("\n[RUNNING] ", ' '.join(cmd))
    
    try:
        # MMAction2 í›ˆë ¨ ì‹¤í–‰
        subprocess.run(cmd, check=True, env=env)
    finally:
        # í›ˆë ¨ ì„±ê³µ/ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if temp_pkl_path_to_delete and os.path.exists(temp_pkl_path_to_delete):
            print(f"\n[CLEANUP] Deleting temporary shuffled PKL: {temp_pkl_path_to_delete}")
            os.remove(temp_pkl_path_to_delete)
            print("[CLEANUP] Cleanup complete.")


if __name__ == '__main__':
    main()