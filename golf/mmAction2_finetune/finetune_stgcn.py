#!/usr/bin/env python
"""
finetune_stgcn.py

ST-GCN fine-tuning using an existing combined annotation PKL
and MMAction2 tools/train.py.
Adds --shuffle option to re-split the PKL data into train/val sets
based on a fixed ratio. The shuffled PKL is temporary and deleted after training.
"""

import argparse
import os
import sys
import pickle
import subprocess
import random
from collections import Counter
import uuid # â­ï¸ [ì¶”ê°€/ìˆ˜ì •] resplit_pkl í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ importí•©ë‹ˆë‹¤.

# --- Global Configuration ---
# MMAction2 ë£¨íŠ¸ ê²½ë¡œ (tools/train.py ì ‘ê·¼ìš©)
MM_ROOT = r"D:\mmaction2"
sys.path.append(MM_ROOT)

# Frequently-used defaults (hardcoded for this task)
DEVICE = "cuda:0"
VAL_SPLIT = "xsub_val" # MMAction2ì˜ split nameì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
RESPLIT_RATIO = 0.1 

# Fixed paths for 5-Class (Default)
DEFAULT_INPUT_PKL = r"D:\golfDataset\crop_pkl\combined_5class.pkl"
DEFAULT_CFG = r"configs\skeleton\stgcnpp\my_stgcnpp.py"

# Fixed paths for 3-Class
THREE_CLASS_INPUT_PKL = r"D:\golfDataset\crop_pkl\combined_3class.pkl"
THREE_CLASS_CFG = r"configs\skeleton\stgcnpp\my_stgcnpp_3class.py"

DEFAULT_PRETRAINED = r"D:\mmaction2\checkpoints\stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d_20221228-cd11a691.pth"
DEFAULT_WORK_DIR = r"D:\work_dirs\finetune_stgcn_shuffle"

# ----------------------------------------------------------------------
# PKL Manipulation Helper Functions (ìˆ˜ì • ì—†ìŒ)
# ----------------------------------------------------------------------

def _get_all_annotation_indices(data: dict) -> list:
    """PKL íŒŒì¼ì—ì„œ ëª¨ë“  annotationì˜ ì¸ë±ìŠ¤(0ë¶€í„° N-1)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    anns = data.get('annotations', [])
    return list(range(len(anns)))

def _update_splits(data: dict, train_indices: list, val_indices: list):
    """PKL ë°ì´í„°ì˜ split ì •ë³´ë¥¼ ìƒˆë¡œìš´ train/val ì¸ë±ìŠ¤ë¡œ ë®ì–´ì”ë‹ˆë‹¤."""
    data['split'] = {
        'xsub_train': train_indices,
        'xsub_val': val_indices
        # Test splitì€ ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    }

def resplit_pkl(input_pkl_path: str, ratio: float, seed: int = 42) -> str:
    """
    ê¸°ì¡´ PKL íŒŒì¼ì˜ ëª¨ë“  annotationì„ ratioì— ë”°ë¼ train/valë¡œ ì¬ë¶„í• í•˜ê³ ,
    ì„ì‹œ PKL íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ê·¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"\n[SHUFFLE] Re-splitting data with validation ratio {ratio}...")
    
    with open(input_pkl_path, 'rb') as f:
        data = pickle.load(f)

    # Use stratified split based on annotation labels to preserve class ratios
    anns = data.get('annotations', [])
    # Build label -> indices map and sample per-label
    def _stratified_split(annotations, val_ratio, seed=42):
        from collections import defaultdict
        label_to_indices = defaultdict(list)
        for idx, ann in enumerate(annotations):
            label_to_indices[ann.get('label')].append(idx)

        val_indices = set()
        random.seed(seed)
        for label, idxs in label_to_indices.items():
            if not idxs:
                continue
            k = int(len(idxs) * val_ratio)
            if k == 0 and val_ratio > 0:
                k = 1
            k = min(k, len(idxs))
            if k > 0:
                sampled = random.sample(idxs, k)
                val_indices.update(sampled)

        train_indices = [i for i in range(len(annotations)) if i not in val_indices]
        val_indices = sorted(list(val_indices))
        return train_indices, val_indices

    train_indices, val_indices = _stratified_split(anns, ratio, seed=seed)
    _update_splits(data, train_indices, val_indices)

    num_total = len(anns)
    pct_train = (len(train_indices) / num_total) * 100.0 if num_total else 0.0
    pct_val = (len(val_indices) / num_total) * 100.0 if num_total else 0.0
    print(f"[SHUFFLE] Total annotations: {num_total}")
    print(f"[SHUFFLE] New Train size: {len(train_indices)} ({pct_train:.1f}%)")
    print(f"[SHUFFLE] New Val size: {len(val_indices)} ({pct_val:.1f}%)")
    
    # ì„ì‹œ PKL íŒŒì¼ ì €ì¥
    tmp_pkl_dir = 'tmp_pkl'
    os.makedirs(tmp_pkl_dir, exist_ok=True)
    # UUIDë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ í•œ ì„ì‹œ íŒŒì¼ ì´ë¦„ ìƒì„±
    temp_pkl_path = os.path.join(tmp_pkl_dir, f"{os.path.basename(input_pkl_path).replace('.pkl', '')}_{uuid.uuid4().hex[:8]}_shuffled.pkl")
    
    with open(temp_pkl_path, 'wb') as f:
        pickle.dump(data, f)
        
    print(f"[SHUFFLE] Temporary shuffled PKL saved to: {temp_pkl_path}")
    return temp_pkl_path

# ----------------------------------------------------------------------
# Main Logic
# ----------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser()
    # ğŸš¨ epochs ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê³  Config íŒŒì¼ì˜ MAX_EPOCHSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # parser.add_argument('--epochs', type=int, default=30, help='max_epochs override')
    parser.add_argument('--test-pkl', required=False, default='',
                        help='(optional) test annotation PKL ê²½ë¡œ (omit to skip test override)')
    parser.add_argument('--shuffle', action='store_true',
                        help=f'PKLì˜ train/val splitì„ ì¬ë¶„í• í•˜ê³  ì„ìŠµë‹ˆë‹¤ (ratio={RESPLIT_RATIO}).')
    # â­ï¸ [ì¶”ê°€] 3-class ëª¨ë“œ ì˜µì…˜ ì¶”ê°€
    parser.add_argument('--three_class', action='store_true',
                        help='3-class ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ê³  í•´ë‹¹ ì„¤ì •/PKL ê²½ë¡œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (num_classes=3)')
    # resume training from last checkpoint in work_dir
    parser.add_argument('--continue', dest='resume', action='store_true',
                        help='Resume training from last checkpoint found in work_dir (reads last_checkpoint file).')
    # override final max epochs (optional)
    parser.add_argument('--epochs', type=int, default=None,
                        help='Optional: override total max epochs for training (e.g. 100)')
    args = parser.parse_args()

    # â­ï¸ [ìˆ˜ì •] í´ë˜ìŠ¤ ëª¨ë“œì— ë”°ë¥¸ ê¸°ë³¸ê°’ ì„¤ì • (ìŠ¤í¬ë¦½íŠ¸ ì´ˆê¸°ì— ê²½ë¡œ ê²°ì •)
    if args.three_class:
        args.input_pkl = THREE_CLASS_INPUT_PKL
        args.cfg = THREE_CLASS_CFG
        # 3-class ëª¨ë“œì¼ ê²½ìš° n_classesë¥¼ 3ìœ¼ë¡œ í•˜ë“œì½”ë”© (ë‚˜ì¤‘ì— ì¶”ë¡  ë¡œì§ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        n_classes_override = 3
    else:
        args.input_pkl = DEFAULT_INPUT_PKL
        args.cfg = DEFAULT_CFG
        # 5-class ëª¨ë“œì¼ ê²½ìš° n_classesë¥¼ 5ë¡œ í•˜ë“œì½”ë”©
        n_classes_override = 5

    # Use module-level defaults for frequently-changed values
    args.pretrained = DEFAULT_PRETRAINED
    args.work_dir = DEFAULT_WORK_DIR
    args.device = DEVICE
    args.val_split = VAL_SPLIT
    
    print(f"[INFO] Config: {args.cfg}, PKL: {args.input_pkl}")
    
    # ----------------------------------------------------------------------
    # 1. PKL íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬: ì…”í”Œ ì—¬ë¶€ì— ë”°ë¼ ìµœì¢… ì‚¬ìš©í•  PKL ê²½ë¡œ ê²°ì • ë° ì„ì‹œ íŒŒì¼ ì¶”ì 
    # ----------------------------------------------------------------------
    
    final_pkl_path = args.input_pkl
    temp_pkl_path_to_delete = None # ì‚­ì œí•  ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥
    
    # ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ PKL íŒŒì¼(í†µí•© annotation)ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.isfile(args.input_pkl):
        raise FileNotFoundError(f"Annotation PKL not found: {args.input_pkl}")
    
    if args.shuffle:
        final_pkl_path = resplit_pkl(args.input_pkl, RESPLIT_RATIO)
        temp_pkl_path_to_delete = final_pkl_path # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥

    print(f"[OK] Using annotation PKL: {final_pkl_path}")
    
    # ----------------------------------------------------------------------
    # 2. í´ë˜ìŠ¤ ìˆ˜ ì¶”ë¡  ë° ì •ë³´ ì¶œë ¥
    # ----------------------------------------------------------------------
    
    # infer number of classes from PKL (if labels exist) and show distribution
    # (í´ë˜ìŠ¤ ì¶”ë¡ ì€ ìµœì¢… PKL íŒŒì¼ë¡œ ìˆ˜í–‰í•´ë„ ë¬´ë°©í•¨)
    with open(final_pkl_path, 'rb') as f: 
        data = pickle.load(f)
    anns = data.get('annotations', [])
    labels = [a.get('label') for a in anns if 'label' in a]
    
    n_classes_inferred = None
    if labels:
        cnt = Counter(labels)
        # í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ 0ë¶€í„° ì‹œì‘í•œë‹¤ê³  ê°€ì •í•˜ê³  ìµœëŒ€ê°’ + 1ì„ ì‚¬ìš©
        n_classes_inferred = max(cnt.keys()) + 1 if cnt else 0 
        print(f"[OK] Detected labels: {dict(cnt)}, Inferred n_classes={n_classes_inferred}")
    else:
        print("[WARN] No 'label' field found in annotations.")
    
    # ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©í•  n_classes ê²°ì • (í•˜ë“œì½”ë”©ëœ ê°’ > ì¶”ë¡ ëœ ê°’)
    n_classes_final = n_classes_override
    if n_classes_inferred is not None and n_classes_inferred != n_classes_override:
        print(f"[WARN] Inferred n_classes ({n_classes_inferred}) does not match override ({n_classes_override}). Using override value.")
    
    print(f"[INFO] Final model.cls_head.num_classes will be set to: {n_classes_final}")

    # ----------------------------------------------------------------------
    # 3. Config íŒŒì¼ ì²˜ë¦¬ ë° ì„ì‹œ ì„¤ì • ìƒì„±
    # ----------------------------------------------------------------------
    
    cfg_path = os.path.join(MM_ROOT, args.cfg.strip())
    if not os.path.isfile(cfg_path):
        raise FileNotFoundError(f"Config not found: {cfg_path}")

    # ì„ì‹œ finetune config ìƒì„± (ì ˆëŒ€ê²½ë¡œ base ìƒì†)
    tmp_cfg_dir = 'tmp_cfg'
    os.makedirs(tmp_cfg_dir, exist_ok=True)
    # â­ï¸ [ìˆ˜ì •] ì„ì‹œ config íŒŒì¼ì— uuidë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ í•œ ì´ë¦„ ë¶€ì—¬ (ì´ì „ ì½”ë“œëŠ” ê³ ì •ëœ ì´ë¦„ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)
    finetune_cfg = os.path.join(tmp_cfg_dir, f"finetune_stgcn_cfg_{uuid.uuid4().hex[:8]}.py") 
    
    # Windows ê²½ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìŠ¬ë˜ì‹œ(/)ë¡œ í†µì¼
    base_cfg_abs = cfg_path.replace('\\', '/')
    
    with open(finetune_cfg, 'w', encoding='utf-8') as f:
        f.write(f"_base_ = ['{base_cfg_abs}']\n")
    print(f"[OK] generated finetune config: {finetune_cfg}")

    # ----------------------------------------------------------------------
    # 4. MMAction2 tools/train.py ì‹¤í–‰ ë° ìë™ ì‚­ì œ
    # ----------------------------------------------------------------------
    
    env = os.environ.copy()
    if args.device.startswith('cuda'):
        env['CUDA_VISIBLE_DEVICES'] = args.device.split(':', 1)[1]

    cmd = [
        sys.executable,
        os.path.join(MM_ROOT, 'tools', 'train.py'),
        finetune_cfg,
        '--work-dir', args.work_dir,
        '--cfg-options',
        # Avoid setting global load_from. Instead set the backbone init_cfg checkpoint.
        f"model.backbone.init_cfg.checkpoint={args.pretrained}",
        
        # â­ï¸ Legacy ì˜¤ë²„ë¼ì´ë“œ ë°©ì‹ (ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìŒ): train/val dataloader.dataset í•˜ìœ„ì˜ ann_file ì˜¤ë²„ë¼ì´ë“œ
        f"train_dataloader.dataset.dataset.ann_file={final_pkl_path}",
        f"val_dataloader.dataset.ann_file={final_pkl_path}", # RepeatDataset ë¯¸ì‚¬ìš© ì‹œ .dataset ìƒëµ
        
        # ê¸°íƒ€ í•™ìŠµ ì„¤ì • (Legacy ì˜¤ë²„ë¼ì´ë“œ ë°©ì‹)
        f"train_dataloader.dataset.dataset.split=xsub_train",
        f"val_dataloader.dataset.split={args.val_split}", # RepeatDataset ë¯¸ì‚¬ìš© ì‹œ .dataset ìƒëµ
    ]

    # If user asked to override epochs, append cfg-option for max epochs
    if args.epochs is not None:
        cmd.extend([f"train_cfg.max_epochs={args.epochs}"])

    # Test PKL ê²½ë¡œê°€ ì£¼ì–´ì§„ ê²½ìš° ì²˜ë¦¬ (Legacy ì˜¤ë²„ë¼ì´ë“œ ë°©ì‹)
    if args.test_pkl:
        if not os.path.isfile(args.test_pkl):
            raise FileNotFoundError(f"Test PKL not found: {args.test_pkl}")
        # Test Dataloaderë„ ê²½ë¡œë¥¼ í•œ ë‹¨ê³„ ëœ ê¹Šê²Œ ì§€ì • (valê³¼ ë™ì¼í•œ êµ¬ì¡°)
        cmd.extend([f"test_dataloader.dataset.ann_file={args.test_pkl}", f"test_dataloader.dataset.split={args.val_split}"])

    # í´ë˜ìŠ¤ ìˆ˜ê°€ ê²°ì •ëœ ê²½ìš°, ëª¨ë¸ í—¤ë“œ ì—…ë°ì´íŠ¸
    if n_classes_final is not None:
        cmd.extend([f"model.cls_head.num_classes={n_classes_final}"])

    # If resume requested, try to find last checkpoint in work_dir (MMEngine writes a `last_checkpoint` file)
    if args.resume:
        resume_ckpt = None
        try:
            lc_path = os.path.join(args.work_dir, 'last_checkpoint')
            if os.path.isfile(lc_path):
                with open(lc_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content:
                        resume_ckpt = content
            # fallback: look for epoch_*.pth in work_dir
            if not resume_ckpt and os.path.isdir(args.work_dir):
                files = [f for f in os.listdir(args.work_dir) if f.endswith('.pth')]
                # prefer epoch_N.pth highest N
                epoch_files = [f for f in files if f.startswith('epoch_')]
                if epoch_files:
                    # sort by epoch number
                    def epoch_num(fn):
                        try:
                            return int(fn.split('epoch_')[-1].split('.pth')[0])
                        except Exception:
                            return -1
                    epoch_files.sort(key=epoch_num, reverse=True)
                    resume_ckpt = os.path.join(args.work_dir, epoch_files[0])
        except Exception:
            resume_ckpt = None

        if resume_ckpt and os.path.isfile(resume_ckpt):
            # MMAction2's train.py expects the --resume option (with a path),
            # not --resume-from. Use --resume to pass the checkpoint path.
            cmd.extend(['--resume', resume_ckpt])
            print(f"Resuming training from checkpoint: {resume_ckpt}")
        else:
            print(f"Resume requested but no checkpoint found in work_dir={args.work_dir}. Continuing from scratch.")

    print("\n[RUNNING] ", ' '.join(cmd))
    
    try:
        # MMAction2 í›ˆë ¨ ì‹¤í–‰
        subprocess.run(cmd, check=True, env=env)
    finally:
        # í›ˆë ¨ ì„±ê³µ/ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if temp_pkl_path_to_delete and os.path.exists(temp_pkl_path_to_delete):
            print(f"\n[CLEANUP] Deleting temporary shuffled PKL: {temp_pkl_path_to_delete}")
            os.remove(temp_pkl_path_to_delete)
            print("[CLEANUP] Temporary shuffled PKL cleanup complete.")
        
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ì •ë¦¬
        if os.path.exists(finetune_cfg):
            os.remove(finetune_cfg)
            print(f"[CLEANUP] Temporary config file deleted: {finetune_cfg}")


if __name__ == '__main__':
    main()