{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52750aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 436 samples found in D:\\golfDataset\\dataset\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings: 100%|███████████████████████| 393/393 [10:55<00:00,  1.67s/it]\n",
      "Test Embeddings: 100%|██████████████████████████| 43/43 [01:13<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings and labels saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")  # timesformer 모듈 경로 추가\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import InterpolationMode, functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# ----------------- 하이퍼파라미터 ----------------------------\n",
    "ROOT          = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "NUM_FRAMES    = 32\n",
    "CLIPS_PER_VID = 5\n",
    "IMG_SIZE      = 224\n",
    "BATCH_SIZE    = 1\n",
    "TEST_RATIO    = 0.1  # 테스트셋 비율\n",
    "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----------------- 재현성 ----------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# ----------------- 전처리 함수 ----------------------------\n",
    "def preprocess_tensor(img_tensor):\n",
    "    img = F.resize(img_tensor, 256, interpolation=InterpolationMode.BICUBIC)\n",
    "    img = F.center_crop(img, IMG_SIZE)\n",
    "    img = F.normalize(img, [0.45]*3, [0.225]*3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def uniform_sample(length, num):\n",
    "    if length >= num:\n",
    "        return np.linspace(0, length - 1, num).astype(int)\n",
    "    return np.pad(np.arange(length), (0, num - length), mode='edge')\n",
    "\n",
    "\n",
    "def load_clip(path: Path):\n",
    "    vr = VideoReader(str(path))\n",
    "    L = len(vr)\n",
    "    seg_edges = np.linspace(0, L, CLIPS_PER_VID + 1, dtype=int)\n",
    "    clips = []\n",
    "    for start, end in zip(seg_edges[:-1], seg_edges[1:]):\n",
    "        idx = uniform_sample(end - start, NUM_FRAMES) + start\n",
    "        arr = vr.get_batch(idx).asnumpy().astype(np.uint8)        # (T, H, W, 3)\n",
    "        clip = torch.from_numpy(arr).permute(0,3,1,2).float()/255.0\n",
    "        clip = torch.stack([preprocess_tensor(f) for f in clip])  # (T,3,H,W)\n",
    "        clips.append(clip.permute(1,0,2,3))                        # (3,T,H,W)\n",
    "    return clips\n",
    "\n",
    "class SwingDataset(Dataset):\n",
    "    def __init__(self, root: Path):\n",
    "        mapping = {\"balanced_true\": 1, \"false\": 0}\n",
    "        self.samples = [(p,l) for sub,l in mapping.items()\n",
    "                        for p in (root/sub/\"crop_video\").glob(\"*.mp4\")]\n",
    "        print(f\"✅ {len(self.samples)} samples found in {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        return torch.stack(load_clip(path)), label\n",
    "\n",
    "# ----------------- 데이터로더 구성 ----------------------------\n",
    "ds_full = SwingDataset(ROOT)\n",
    "n_test = int(len(ds_full)*TEST_RATIO)\n",
    "n_train = len(ds_full)-n_test\n",
    "train_ds, test_ds = random_split(ds_full, [n_train,n_test])\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=0, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=0, pin_memory=True)\n",
    "\n",
    "# ----------------- 모델 로드 및 헤드 교체 ----------------------------\n",
    "PRETRAIN_PYTH = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_96x4_224_K600.pyth\")\n",
    "model = TimeSformer(\n",
    "    img_size=IMG_SIZE, num_frames=NUM_FRAMES,\n",
    "    num_classes=2, attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRETRAIN_PYTH)\n",
    ").to(device)\n",
    "\n",
    "# 외부 wrapper head 제거\n",
    "model.head = nn.Identity()\n",
    "if hasattr(model, 'cls_head'):\n",
    "    model.cls_head = nn.Identity()\n",
    "# 내부 ViT head 제거\n",
    "model.model.head = nn.Identity()\n",
    "if hasattr(model.model, 'cls_head'):\n",
    "    model.model.cls_head = nn.Identity()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ----------------- 임베딩 추출 ----------------------------\n",
    "all_train_embs, all_train_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(train_ld, desc=\"Train Embeddings\", ncols=80):\n",
    "        clips = clips.squeeze(0).to(device)           # (CLIPS_PER_VID,3,T,H,W)\n",
    "        # 내부 ViT feature extraction\n",
    "        feats = model.model.forward_features(clips)   # (CLIPS_PER_VID, num_patches+1, D) or (CLIPS_PER_VID, D)\n",
    "        if feats.ndim == 3:\n",
    "            cls_embs = feats[:, 0, :]                # (CLIPS_PER_VID, D)\n",
    "        else:\n",
    "            cls_embs = feats                         # (CLIPS_PER_VID, D)\n",
    "        emb = cls_embs.mean(dim=0).cpu().numpy()      # (D,)\n",
    "        all_train_embs.append(emb)\n",
    "        all_train_labels.append(label.item())\n",
    "\n",
    "all_test_embs, all_test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for clips, label in tqdm(test_ld, desc=\"Test Embeddings\", ncols=80):\n",
    "        clips = clips.squeeze(0).to(device)\n",
    "        feats = model.model.forward_features(clips)\n",
    "        if feats.ndim == 3:\n",
    "            cls_embs = feats[:, 0, :]\n",
    "        else:\n",
    "            cls_embs = feats\n",
    "        emb = cls_embs.mean(dim=0).cpu().numpy()\n",
    "        all_test_embs.append(emb)\n",
    "        all_test_labels.append(label.item())\n",
    "\n",
    "# ----------------- 저장 ----------------------------\n",
    "np.save(r\"embbeding_data\\timesformer\\train_embeddings.npy\", np.stack(all_train_embs))\n",
    "np.save(r\"embbeding_data\\timesformer\\train_labels.npy\",   np.array(all_train_labels))\n",
    "np.save(r\"embbeding_data\\timesformer\\test_embeddings.npy\",  np.stack(all_test_embs))\n",
    "np.save(r\"embbeding_data\\timesformer\\test_labels.npy\",     np.array(all_test_labels))\n",
    "\n",
    "print(\"✅ Embeddings and labels saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b89dee",
   "metadata": {},
   "source": [
    "## 1개 데이터 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2a4794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 436 samples found in D:\\golfDataset\\dataset\\train\n",
      "[CLS] 임베딩 차원 D: 768\n",
      "클립 개수 (CLIPS_PER_VID): 5\n",
      "[CLS] 토큰 임베딩 차원 D: 768\n",
      "최종 평균 임베딩 shape: (768,)\n",
      "레이블: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\timesformer\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from decord import VideoReader\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "# (중략) 기존 SwingDataset, load_clip, preprocess_tensor 등 정의 부분은 그대로 사용\n",
    "\n",
    "# 하이퍼파라미터\n",
    "ROOT          = Path(r\"D:\\golfDataset\\dataset\\train\")\n",
    "NUM_FRAMES    = 32\n",
    "CLIPS_PER_VID = 5\n",
    "IMG_SIZE      = 224\n",
    "TEST_RATIO    = 0.1\n",
    "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) 데이터셋 준비\n",
    "ds_full = SwingDataset(ROOT)\n",
    "n_test  = int(len(ds_full) * TEST_RATIO)\n",
    "n_train = len(ds_full) - n_test\n",
    "train_ds, _ = random_split(ds_full, [n_train, n_test])\n",
    "\n",
    "# 2) 모델 로드 & head 교체\n",
    "PRE_PTH = Path(r\"D:\\timesformer\\pretrained\\TimeSformer_divST_96x4_224_K600.pyth\")\n",
    "model = TimeSformer(\n",
    "    img_size=IMG_SIZE,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    num_classes=2,\n",
    "    attention_type='divided_space_time',\n",
    "    pretrained_model=str(PRE_PTH)\n",
    ").to(device)\n",
    "# 외부 wrapper 헤드 제거 (남겨둬도 무해)\n",
    "model.head = nn.Identity()\n",
    "if hasattr(model, 'cls_head'):\n",
    "    model.cls_head = nn.Identity()\n",
    "\n",
    "# ↓ 핵심: 내부 ViT에도 head Identity 적용\n",
    "model.model.head = nn.Identity()\n",
    "if hasattr(model.model, 'cls_head'):\n",
    "    model.model.cls_head = nn.Identity()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 3) 첫 번째 샘플만 추출\n",
    "clips, label = train_ds[0]            # clips: (CLIPS_PER_VID, 3, T, H, W), label: int\n",
    "clips = clips.to(device)              # GPU로 옮기기\n",
    "\n",
    "# 4) 임베딩 계산\n",
    "\n",
    "with torch.no_grad():\n",
    "    clips = clips.squeeze(0).to(device)       # (5,3,T,H,W)\n",
    "\n",
    "    # 1) 내부 ViT으로부터 features 추출\n",
    "    feats = model.model.forward_features(clips)\n",
    "\n",
    "    # 2) feats가 3D면 [CLS] 토큰(0번)만, 2D면 그대로 사용\n",
    "    if feats.ndim == 3:\n",
    "        cls_embs = feats[:, 0, :]            # (5, D)\n",
    "    else:\n",
    "        cls_embs = feats                     # already (5, D)\n",
    "\n",
    "    # 3) 클립별 평균 → (D,)\n",
    "    emb = cls_embs.mean(dim=0).cpu().numpy()\n",
    "\n",
    "print(f\"[CLS] 임베딩 차원 D: {emb.shape[0]}\")\n",
    "\n",
    "\n",
    "# 고친 부분\n",
    "print(f\"클립 개수 (CLIPS_PER_VID): {cls_embs.shape[0]}\")\n",
    "print(f\"[CLS] 토큰 임베딩 차원 D: {cls_embs.shape[1]}\")\n",
    "print(f\"최종 평균 임베딩 shape: {emb.shape}\")\n",
    "print(f\"레이블: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
